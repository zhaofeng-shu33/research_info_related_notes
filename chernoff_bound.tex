\documentclass{article}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amssymb}
\title{Derivation of Chernoff information bound for Hypothesis Testing Problem}
\begin{document}
\maketitle
This derivation can be found from 11.9 of \cite{it}.

Notation: $\bm{x} = (x_1, \dots, x_n)$ is a vector. The samples $x_i$ are i.i.d. either from $P_1$ or $P_2$.

$ H_1: X \sim P_1 $ versus $ H_2:  X \sim P_2 $.

Decision rule of accepting $H_1$:
\begin{equation}
A=\{\bm{x}: P_1(\bm{x}) > P_2(\bm{x})\}
\end{equation}

The error probability $P_e$ can be written as:
\begin{align*}
P_e & = \sum_{\bm{x} \in A^c} P_1(\bm{x})
+ \sum_{\bm{x} \in A} P_2(\bm{x}) \\
& = \sum_{\bm{x} \in \mathcal{X}^n} \min \{ P_1(\bm{x}), P_2(\bm{x})\} \\
\end{align*}
Choosing $ 0 \leq \lambda \leq 1 $,
we then have
\begin{align*}
P_e & \leq \sum_{\bm{x} \in \mathcal{X}^n}  P_1^{\lambda}(\bm{x})  P_2^{1-\lambda}(\bm{x}) \\
& =  \sum_{\bm{x} \in \mathcal{X}^n}  \prod_{i=1}^n P_1^{\lambda}(x_i)  P_2^{1-\lambda}(x_i) \\
& = \prod_{i=1}^n \sum_{x_i\in \mathcal{X}} P_1^{\lambda}(x_i)  P_2^{1-\lambda}(x_i) \\
& = \left(\sum_{x\in \mathcal{X}} P_1^{\lambda}(x)  P_2^{1-\lambda}(x)\right)^n
\end{align*}
To maximize $\sum_{x\in \mathcal{X}} P_1^{\lambda}(x)  P_2^{1-\lambda}(x)$ we take the derivative
and find that the optimal $\lambda^*$ satisfies
\begin{equation}
\sum_{x\in \mathcal{X}} P_{\lambda^*}(x) \log \frac{P_1(x)}{P_2(x)} = 0 \textrm{ where }
P_{\lambda}(x) = \frac{P_1^{\lambda}(x)P_2^{1-\lambda}(x)}{\sum_{a\in \mathcal{X}}P_1^{\lambda}(a)P_2^{1-\lambda}(a)}
\end{equation}
which is equivalent to say $\lambda^*$ is a solution to $D(P_{\lambda}|| P_1) = D(P_{\lambda} || P_2)$.

Since $\sum_{x\in \mathcal{X}} P_1^{\lambda^*}(x)  P_2^{1-\lambda^*}(x) = \frac{P_1^{\lambda^*}(y)P_2^{1-\lambda^*}(y)}
{P_{\lambda^*}(y)}$ for any $y\in \mathcal{X}$. Therefore,
\begin{align*}
-\log \sum_{x\in \mathcal{X}} P_1^{\lambda^*}(x)  P_2^{1-\lambda^*}(x) &= \sum_{y \in \mathcal{X}} P_{\lambda^*}(y) \left(-\log \sum_{x\in \mathcal{X}} P_1^{\lambda^*}(x)  P_2^{1-\lambda^*}(x) \right)\\
& = \sum_{y \in \mathcal{X}} P_{\lambda^*}(y) \log\left(\frac{P_{\lambda^*}(y)}{P_1^{\lambda^*}(y)P_2^{1-\lambda^*}(y)}\right) \\
& = \lambda^* D(P_{\lambda^*} || P_1) + (1-\lambda^*) D(P_{\lambda^*} || P_2) \\
& = D(P_{\lambda^*} || P_1) \textrm{ or } D(P_{\lambda^*} || P_2)
\end{align*}
\begin{thebibliography}{9}
	\bibitem{it} Cover, Thomas M. Elements of information theory. John Wiley \& Sons, 1999.
\end{thebibliography}
\end{document}