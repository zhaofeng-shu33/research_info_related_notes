\documentclass{article}
\usepackage{amsmath}

\title{The statistical problem of correlation as variational and eigenvalue problem, including its connection with the curve fitting\footnote{translated by Feng Zhao, the title of the original article is "Das statistische Problem der Korrelation als Variations- und Eigenwertproblem und sein Zusammenhang mit der Ausgleichsrechnung"}}

\date{December 2020}
\author{Hans Gebelein}

\begin{document}
\maketitle
\begin{centering}
A satisfied correlation metric is required to satisfy
the property that the two random variables are independent when the metric is zero and one is
determined by another when the metric is one.
The task to obtain such correlation metric is conducted
as a variational problem and can be transformed to solve
the smallest eigenvalue of a homogeneous Fredholm integral equation. By examining theses variational
problems we can obtain its relationship with the commonly used correlation metric and the curve fitting.
\end{centering}

\section{Problem formulation and commonly-used correlation metric}
One of the notable view for the judgement of two-dimensional probabilistic distribution $w(x,y)$ is by fixing one variable $x$, the distribution of $y$ is more or less influenced or not. As is well known,
Such influence does not exist when $w(x,y)$
is the product of a function $w_1(x)$ of $x$ and
a function $w_2(y)$ of $y$. $x$ and $y$
are mutually independent in such case. On the
other hand, it can happen that for each given $x$
only a single $y$ is corresponded. Then $y$
is a function of $x$ and we say a complete correlation
exists between the two variables. In general the result lies between the two extreme cases, and there is a question about a metric for the tightness of the relationship
between $x$ and $y$. This is the correlation problem of statistics.

To characterize the correlation between $y$ and $x$,
there are many well-known different metrics, which we firstly quote here. Especially we observe a so-called
geometric probabilistic distribution for the random variable $x$ and $y$. Such pair is determined by the
positive function $w(x,y)$ by the normalization condition:
\begin{equation}\label{eq:wxy}
    \iint w(x,y) dx dy = 1
\end{equation}
Integrating by $x$ or $y$ we can obtain
\begin{equation}
    w_1(x) = \int w(x,y)dy \textrm{ and }
    w_2(y) = \int w(x,y)dx
\end{equation}
Their integral about $x$ or $y$ is 1. In the following we use $a$
and $b$ to describe the mean value of $x$ and $y$
with respect to the distribution $w(x,y)$.
This is the center or mass coordinate of the mass
density on the $xy$-plane. It is
\begin{align}
    a = \iint x w(x,y) dx dy = \int x w_1(x)dx \notag \\
    b = \iint y w(x,y) dx dy = \int y w_2(y)dy \label{eq:ab}
\end{align}
Further $s^2$ and $t^2$ mean the stastical dispersion of $x$ around its mean value $a$
or $y$ around its mean value $b$ respectively.
These are moment of inertia of the plane with $w(x,y)$ as the density around the axis through the
center of mass and parallel to the coordinate axis. It is defined as:
\begin{align}
    s^2 = \iint (x-a)^2 w(x,y) dx dy = \int (x-a)^2 w_1(x)dx \notag \\
    t^2 = \iint (y-b)^2 w(x,y) dx dy = \int (y-b)^2 w_2(y)dy 
    \label{eq:st}
\end{align}
As the correlation metric we need consider a numerical quantity, which should satisfy three properties.
Firstly it can be computed from any given two-parameter distribution. Secondly the value should
be zero is $x$ and $y$ are statistically independent. Thirdly the value should be one is $x$ and $y$
are fully dependent.

The classtical correlation coefficient satisfies the first and second condition, which is based on the moment of deviation
of the distribution:
\begin{equation}
r = \frac{1}{st} \iint (x-a)(y-b)w(x,y)dxdy
\end{equation}
We always have $r^2 \leq 1$.
The value $r=+ 1$ occurs when $y$ has positive linear relationship with $x$.
The value $r= - 1$ occurs when $y$ has negative linear relationship with $x$.
The disadvantage of this correlation coefficient is that based on $r=0$ we cannot get the
full independence condition $w(x,y)=w_1(x)w_2(y)$.
On the other hand, when $x$ and $y$ have full statistical non-linear dependence, $r$
is different from one.

To remedy the second shortcoming other correlation metrics are proposed.
A unified method to the detailed analysis of the distribution $w(x,y)$ are the regression line $\bar{y}(x)$ and $\bar{x}(y)$.
They are the mechanical view of the geometric place for the center of mass of the strip in
the $xy$-plane parallel to the coordinate axes.
\begin{align}
   \bar{y}(x) = \frac{\int y w(x,y) dy}{\int w(x,y) dy} = \frac{1}{w_1(x)} \int y w(x, y)dy \notag \\
   \bar{x}(y) =  \frac{\int x w(x,y) dx}{\int w(x,y) dx} = \frac{1}{w_2(x)} \int x w(x, y)dx 
\end{align}
In the two extreme cases of statistical independence and
full dependence, the two regression lines are clearly
characterized. For independence we have $\bar{y}(x) = b$
and $\bar{x}(y)=a$ due to $w(x,y) = w_1(x)w_2(y)$.
That is, the regression lines are the horizontal and
the vertical straight lines through the center of mass of the
distribution. It should be noted that from this condition of regression lines,
the center of mass cannot be inferred reversely based on the assumption of statistical independence.
Actually we cannot deduce $w(x,y)=w_1(x)w_2(y)$.

On the other hand, under
the condition of full dependence, $x$ and
$y$ are determined by the function $y(x)$
and $x(y)$ explicitly. We have $\bar{y}(x) = y(x)$
and $\bar{x}(y) = x(y)$ so that in this case
the two regression lines coincide.

In statistical practice, based on the regression lines,
Pearson introduced \textsf{correlation ratio}.
\begin{align}
   k^2_{yx} = \frac{1}{t^2}\int (\bar{y}(x) - b)^2 w_1(x)dx \textrm{ (correlation ratio of $y$ to $x$}) \notag \\
   k^2_{xy} = \frac{1}{s^2}\int (\bar{x}(y) - a)^2 w_2(y)dy \textrm{ (correlation ratio of $y$ to $x$})
\end{align}
These correlation ratio take value zero at independence
condition and in particular take value one at full
dependence condition. Therefore, the measure of correlation with the help of this quantity is very satisfied when they are equal. If we are not dealing with the two extreme
cases with $k^2_{yx} = k^2_{xy}=0$ or $1$, then
generally $k^2_{yx}$ is not equal to $k^2_{xy}$. Only when the regression lines are straight, the two correlation coefficients take the same value, and this
value also equal to the square of the correlation
coefficient $r$.

Pearson also suggested another measure of correlation metric as \textsf{Mean square Contingency}.
\begin{align}
   f^2 &= \frac{1}{\sqrt{(m-1)(n-1)}}\sum_{ik}
   \frac{(w(x_i,y_k)-w_1(x_i)w_2(y_k))^2}{w_1(x_i)w_2(y_k)} \notag \\
   &=\frac{1}{\sqrt{(m-1)(n-1)}}\left(\sum_{ik}
   \frac{w^2(x_i,y_k)}{w_1(x_i)w_2(y_k)} -1\right)
\end{align}
Here the discrete distribution must be assumed, in
which $m$ and $n$ are the number of rows and columns of the so-called correlation table. Then
under the condition of statistical independence $w(x_i,y_k) = w_1(x_i)w_2(y_k)$, $f^2=0$ is obvious.
Under the condition of full dependence, $m=n$ is necessary. $w(x_i,y_k)$ has only $n$ non-zero
probability values
$$
w(x_i, y_k) = w_1(x_i) = w_2(y_k)
$$
Then there are $n$ non-zero terms $w^2(x_i, y_k)/w_1(x_i)w_2(y_k)$, which are all one, and due
to $m=n$ we have $f^2=1$.

The particular advantage of this correlation metric
of Pearson is that the value depends only on the
frequency, not on the value of $x$ or $y$.
Hence this metric also allows other non-numerical
quantities like color, gender etc. Its disadvantage is
that $f^2$ is defined only for the discrete distribution and we will show later that the natural generalization to $w(x,y)$ is not possible in general.

All these existing correlation metrics have special
advantages and disadvantages. Their relationship with
each other is not easily observed. In the following
we show that the correlation problem can be formulated
as a variational problem, and this variational problem
can be transformed to the homogeneous Fredholm integral equation, where we can solve the eigenvalue problem.
We find that the commonly used correlation metrics are not others but different method to approximate the smallest eigenvalue of this integral equation.
If we are dealing with a discrete distribution, then the integral equation degenerates to a homogeneous linear equation system.

The question about the best relation between $y$ and
$x$ is also met in curve fitting, which deals with
a sequence of observed points by finding a simple curve
to best fit the data. We deal with the curve fitting problem by treating it as a correlation problem, which provides new point of view for the solution. It shows that from correlation problem, we can develop good methods to solve the task of curve fitting.
\section{Variational problem and integral equation}
All the existing correlation metrics have certain shortcomings. Without consideration of calculability, now we ask the question about a theoretical perfect and conceptual simple correlation metric.
With respect to the conceptual simplicity, the correlation coefficient is the most satisfying.
Its main disadvantage is that under non-linear transformation of the random variable $x$
and $y$, the quantity is not invariant.
If the two quantities are assigned other values,
that is, if $x$ is transformed by $f(x)$ and $y$
is transformed by $g(y)$, then the value of $r$
varies in general. The disadvantage of the quantity $r$
is removed when all these kinds of transformations
are considered. To obtain an improved correlation metric,
we inquire for the greatest value which $r$ can achieve by
such transformation. Then we have the following variational problem:

Given a positive function $w(x,y)$ which satisfies the
normalization condition $\iint w(x,y)dxdy=1$. It is to determine the function $f(x)$ and $g(y)$ such that
\begin{equation}
    K^2 = \max\frac{(\iint f(x)g(y)w(x,y)dxdy)^2}
    {\iint f^2(x)w(x,y)dxdy \iint g^(y) w(x,y)dxdy}
\end{equation}
Then $K^2$ is a measure for the correlation of the distribution
$w(x,y)$. In the term of mechanics,
the correlation measure is the largest value of the quotient
of the moment of deviation divided by the two moments of inertia for the distribution when all kinds of distortion in $x$ or $y$ direction are allowed.

We can write the variational problem in a simpler way when 
only normalized functions $f(x)$ and $g(y)$ are allowed.
That is:
\begin{align}
\iint f^2(x) w(x,y)dxdy &= \iint f^2(x)w_1(x)dx=1\notag\\
\iint g^2(x) w(x,y)dxdy &= \iint g^2(x)w_2(y)dy=1 \label{eq:fgw}
\end{align}
Functions like $f(x)=1, f(x)=\frac{x-a}{s}$
and $g(y)=1, g(y)=\frac{y-b}{t}$ satisfy \eqref{eq:fgw}
where $a,b,s,t$ are defined in (\ref{eq:ab}, \ref{eq:st}).
Owing to this constraint the variational problem of correlation becomes:
\begin{equation}
    K^2 = (\max \iint f(x)g(y)dxdy)^2
\end{equation}
To guarantee this variational problem meaningful, another
side condition is necessary. Since $w(x,y)\geq 0$, by
Cauchy-Schwarz inequality
\begin{align*}
    (\iint f(x)g(y)w(x,y)dxdy)^2
    &= (\iint f(x)\sqrt{w(x,y)} \cdot g(y)\sqrt{w(x,y)}dxdy)^2\\
    &\le \iint f^2(x)w(x,y)dxdy\cdot 
    \iint g^2(x)w(x,y)dxdy = 1
\end{align*}
From this estimation the equality holds when the function
$f(x)\sqrt{w(x,y)}$ and $g(y)\sqrt{w(x,y)}$ are proportional.
This holds for any $w(x,y)$ when $f(x)=g(y)=1$. This case is trivial from \eqref{eq:wxy} and should be excluded. Therefore, for the variational problem we only allow
functions which are orthogonal to $f(x)=1$ (or $g(y)=1$)
with respect to $w_1(x)$ (or $w_2(y)$). This condition goes
as
\begin{align}
    \iint f(x)w(x,y)dxdy = \int f(x)w_1(x)dx=0 \notag\\
        \iint g(x)w(x,y)dxdy = \int g(y)w_2(y)dy=0
\end{align}

\end{document}






