\documentclass{article}
\usepackage{ctex}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage{bm}
\DeclareMathOperator\E{\mathbb{E}}
\DeclareMathOperator\Var{\mathrm{Var}}
\DeclareMathOperator{\Bern}{Bern}
\DeclareMathOperator{\Pois}{Pois}
\newtheorem{lemma}{引理}
\theoremstyle{definition}
\newtheorem{definition}{定义}
\newtheorem{example}{例}
\usepackage{mathtools}
\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiter{\ip}{\langle}{\rangle}
\begin{document}
\title{概念论常用不等式}
\author{zhaofeng-shu33}
\maketitle
\section{Markov 不等式}
设 $X$ 是非负型随机变量，则 $\Pr(X\geq a) \leq {1 \over a} \E[X]$
若$X$ 不一定非负，可通过$e$指数的形式运用Markov不等式：$\Pr(X\geq a) = \Pr(e^{tX} \geq e^{ta}) \leq { 1 \over \exp(ta)} \E[\exp(tX)]$
(后者被称为 Chernoff 界)

\section{Chebyshev 不等式}
$\Pr(\abs{X-\E[X]} \geq a) \leq {1 \over a^2} \Var[X]$
\section{Hoeffding 不等式}
设$X_i$ 独立，$\Pr(a_i \leq X_i \leq b_i) = 1, S_n = X_1 + \dots + X_n $则有
\begin{align}
\label{eq:Hoeffding1}\Pr( S_n - \E[S_n] \geq t) & \leq \exp \left( - \frac{2t^2}{\sum_{i=1}^n (b_i - a_i)^2} \right) \\
\label{eq:Hoeffding2}\Pr( \abs{S_n - \E[S_n] } \geq t) & \leq 2\exp \left( - \frac{2t^2}{\sum_{i=1}^n (b_i - a_i)^2} \right)
\end{align}
如果~\eqref{eq:Hoeffding1} 成立，对$X'_i= -X_i$ 应用~\eqref{eq:Hoeffding1}有
$$\Pr( S_n - \E[S_n]  \leq -t )  \leq \exp \left( - \frac{2t^2}{\sum_{i=1}^n (b_i - a_i)^2} \right)$$
因此~\eqref{eq:Hoeffding2} 成立。
为证~\eqref{eq:Hoeffding1}, 使用 Hoeffding 引理
\begin{lemma}
设$\E[X]=0, \Pr( a \leq X \leq b) = 1 $，则
\begin{equation}
\E[ \exp(\lambda X) ] \leq \exp\left({\lambda^2 (b-a)^2\over 8}\right)
\end{equation}
\end{lemma}
\begin{proof}[Hoeffding 不等式~\eqref{eq:Hoeffding1} 证明]
\begin{align*}
\Pr( S_n - \E[S_n] \geq t) & \leq \exp(-st) \E[\exp(s(S_n - \E[S_n]) ] \\
& \leq \exp(-st) \prod_{i=1}^n \E[\exp(s(X_i -\E[X_i]))] \\
& \leq \exp(-st) \prod_{i=1}^n \exp({s^2 (b_i - a_i)^2 \over 8 } ) \\
& = \exp(-st + \sum_{i=1}^n { s^2 (b_i - a_i)^2 \over 8} ) 
\end{align*}
取 $ s = {4t \over \sum_{i = 1}^n (b_i - a_i)^2}$ 即得到~\eqref{eq:Hoeffding1} 右端。
\end{proof}
\section{Other useful inequalities}
Falls $X_1, \dots, X_n$ unabhängige Bernoullißverteilte Zufallsgrößen mit
Parameter $p \in (0,1) $ sind, dann gilt
\begin{equation}\label{eq:3delta}
P(\sum_{i=1}^n X_i \geq (1+ \delta) pn ) \leq \exp(-\frac{\delta^2}{3} pn),
\delta \in [0,1], n \in \mathbb{N}
\end{equation}
\begin{proof}
Using Chernoff inequality we can get a tight upper bound
as
$$
P(\sum_{i=1}^n X_i \geq (1+ \delta) pn )  \leq \exp(-tn(1+\delta) p + n\log(1-p+pe^t)).
$$
Then use the Taylor approximation $\log(1+x) \leq x$ we have
$$
 \exp(-t(1+\delta) p + \log(1-p+pe^t)) \leq \exp(np(-1+e^t - t(1+\delta)).
$$
We choose $t$ to minimize the right hand side: $t=\log(1+\delta)$, thus getting
$$
P(\sum_{i=1}^n X_i \geq (1+ \delta) pn )  \leq \exp(np (\delta - (1+\delta)\log(1+\delta)).
$$
\end{proof}
Using the inequality $\log(1+\delta) \geq \frac{2\delta}{2+\delta} $ we
can get
\begin{align*}
\delta - (1+\delta)\log(1+\delta) \leq & \delta (1-\frac{2(1+\delta)}{2+\delta}) \\
& = -\frac{\delta^2}{2+\delta} \leq -\frac{1}{3} \delta^3 \textrm{ since } \delta \leq 1
\end{align*}
Therefore, \eqref{eq:3delta} holds.

Let $X_1, X_2, \dots, $ be i.i.d random variable, with zero mean and variance $\sigma^2$.
$S_n = \sum_{i=1}^n X_i$.
By central limit theorem:
$$
\lim_{n\to \infty} P(\frac{1}{\sqrt{n\sigma^2}} S_n \geq C) =
\int_{C}^{+\infty} \frac{e^{-x^2/2}}{\sqrt{2\pi}} dx
\textrm{ for any } C \in \mathbb{R}
$$
Let $C=\frac{x\sqrt{n}}{\sigma}, x>0$, we can estimate the probability
$P(\frac{S_n}{n} > x)$ by $\int_{\frac{x\sqrt{n}}{\sigma}}^{+\infty}  \frac{e^{-u^2/2}}{\sqrt{2\pi}}du
= \frac{\sigma}{x\sqrt{n}} \exp(-\frac{nx^2}{2\sigma^2})(1+o(1))$.


\section{ f-divergence and its variational representation}
We use discrete distribution as an illustration.
Let $f:(0,\infty) \to \mathbb{R}$ be a convex function and $f(1)=0$.
Let $P,Q$ be two probability distributions on a measurable space
$(\mathcal{X}, \mathcal{F})$. The f-divergence of $Q$ from $P$
is defined as
\begin{equation}
D_f(P||Q) = \sum_{x\in \mathcal{X}} q(x) f\left(\frac{p(x)}{q(x)}\right)
\end{equation}
Based on Legendre transformation, we can give the variational
representation of $D_f(P||Q)$ as
\begin{equation}\label{eq:Df}
D_f(P||Q) = \sup_{t: \mathcal{X} \to R} \left(\mathbb{E}_P[t(X)] - \mathbb{E}_Q[f^*(t(X))]
\right)
\end{equation}
where $f^*$ is the convex conjugate of $f$.

\eqref{eq:Df} can be proved by considering
$f(\frac{p(x)}{q(x)}) = \sup_{t=t(x)} [t\frac{p(x)}{q(x)} - f^*(t)] $
for each $x\in \mathcal{X}$. Then summing up $\sum_{x\in\mathcal{X}} f(\frac{p(x)}{q(x)}) q(x)$
we can get \eqref{eq:Df}.

Choosing $f(x)=x\log x, f^*(t)=e^{t-1}$, the f-divergence reduces to KL-divergence and
\eqref{eq:Df} becomes
\begin{equation}\label{eq:DfKL}
D(P||Q) = \sup_{t: \mathcal{X} \to R} \mathbb{E}_P[t(X)] - \mathbb{E}_Q[\exp(t(X)-1)]
\end{equation}
Treating $t(X)-1$ as an entity, we can rewrite \eqref{eq:DfKL}
as
\begin{equation}\label{eq:DfKL2}
D(P||Q) = \sup_{t: \mathcal{X} \to R} \mathbb{E}_P[t(X)] + 1 - \mathbb{E}_Q[\exp(t(X))]
\end{equation}

A tighter variational representation of KL divergence is given 
by Donsker-Varadhan theorem as:
\begin{equation}\label{eq:DfKL_tight}
D(P||Q) = \sup_{t: \mathcal{X} \to R} \mathbb{E}_P[t(X)] - \log\mathbb{E}_Q[\exp(t(X))]
\end{equation}
using $\log x \leq x-1$ we can show that
$\mathbb{E}_P[t(X)] - \log\mathbb{E}_Q[\exp(t(X))]
\geq \mathbb{E}_P[t(X)] - \mathbb{E}_Q[\exp(t(X)-1)]$.

Comparing \eqref{eq:DfKL2} with \eqref{eq:DfKL_tight}, it is more clearly to see that 
\eqref{eq:DfKL2} is the first-order expansion of \eqref{eq:DfKL_tight} at $\mathbb{E}_Q[\exp(t(X))]=1$.

Compared with \eqref{eq:Df}
, a tighter representation for f-divergence is given in Theorem 1 of
\cite{imp}.
\begin{equation}\label{eq:Dft}
D_f(P||Q) = \sup_{t: \mathcal{X} \to \mathbb{R}} \mathbb{E}_P[t(X)] -  (\mathbbm{1}^R_{f,Q})^* (t)
\end{equation}
The joint convex conjugate of $f,Q$ is defined as $g(t) := (\mathbbm{1}^R_{f,Q})^*(t) $
\begin{equation}
g(t) = \sup_{\substack{r: \mathcal{X} \to \mathbb{R} \\ \mathbb{E}_Q[r(X)] = 1}} \ip{t,r}_Q - \mathbb{E}_Q[f(r(X))] 
\textrm{ for } t: \mathcal{X} \to \mathbb{R}
\end{equation}
The inner product $\ip{t,r}_Q = \sum_{x\in \mathcal{X}} t(x)r(x)Q(x)$.
Choosing $f(x)=x\log x$, we can show that
\eqref{eq:Dft} reduces to
\eqref{eq:DfKL_tight} (using Lagrange multiplier).
\section{New variational representation of KL divergence}
Xiangxiang Xu derives the following expression of KL divergence.
\begin{equation}\label{eq:DfKL3}
D(P||Q) = \sup_{t: \mathcal{X} \to R} \mathbb{E}_P[t(X)] - \mathbb{E}_Q[\exp(t(X))] +1
+\frac{1}{2}(\mathbb{E}_Q[\exp(t(X))]-1)^2
\end{equation}
Comparing \eqref{eq:DfKL3} with \eqref{eq:DfKL_tight}, it is more clearly to see that 
\eqref{eq:DfKL2} is the second-order expansion of \eqref{eq:DfKL_tight} at $\mathbb{E}_Q[\exp(t(X))]=1$.

It is expected that any $n$-th order expansion of \eqref{eq:DfKL_tight} at $\mathbb{E}_Q[\exp(t(X))]=1$ is a valid variational representation of $D(P||Q)$.

\section{Variation representation with simple form}
Suppose the f-divergence has the following representation:
\begin{equation}\label{eq:DfPQr}
D_f(P,Q) = \sup_{\phi: \mathcal{X} \to R} C_1 \mathbb{E}_P(\phi) + C_2 \mathbb{E}_Q(\gamma_2(\phi))
\end{equation}
where $C_1, C_2$ are constants.
We will show that \eqref{eq:DfPQr} reduces to \eqref{eq:Df} under approximate assumptions.
We first consider the special case when $f(x)=x\log x$ and the f-divergence is KL-divergence.
The supremum is reached when $\phi = u$ so that we have
\begin{equation}\label{eq:uPQ}
D_f(P, Q) = C_1 \mathbb{E}_P(u) + C_2 \mathbb{E}_Q(\gamma_2(u))
\end{equation}
which holds for any distributional measure $P, Q$.
Using Lagrange multiplier method (we assume the multipliers are constant values), the constraint gradient of \eqref{eq:uPQ} vanishes,
and we have the following equation system
\begin{align}
C_2 \gamma_2(u_i) & = -\frac{p_i}{q_i} + \lambda_1 \label{eq:deriv1}\\
C_1 u_i & = \log \frac{p_i}{q_i} + \lambda_2 +1 \label{eq:deriv2}\\
C_1 p_i + C_2 q_i \gamma'_2(u_i) &= 0 \label{eq:deriv3}
\end{align}
By combining (\ref{eq:deriv1}, \ref{eq:deriv3}) and canceling out $\frac{p_i}{q_i}$,
we can obtain $C_2 \gamma_2(t) = \frac{C_2}{C_1} \gamma'_2(t) + \lambda_1$.
Thus, $\gamma_2(t)= \frac{\lambda_1}{C_2} + A e^{C_1 t}$.
The coefficient $A$ is determined by combining (\ref{eq:deriv1}, \ref{eq:deriv2}), from which
follows $A=-\frac{e^{-\lambda_2 - 1}}{C_2}$. A final constraint is to make sure the equality
in \eqref{eq:uPQ} holds, and we can obtain from that $\lambda_1 + \lambda_2 = 0$. 
Once solving out $\gamma_2(t) = \frac{-\lambda_2}{C_2} - \frac{e^{-\lambda_2 -1+ C_1t}}{C_2}$,
we can rewrite \eqref{eq:DfPQr} for the special KL-divergence case as
\begin{equation}\label{eq:DfPQKL}
D_{\textrm{KL}}(P, Q) = \sup_{\phi: \mathcal{X} \to R} C_1 \mathbb{E}_P(\phi) -\lambda_2  - \mathbb{E}_Q(e^{-\lambda_2 -1 + C_1 \phi})
\end{equation}
after treating $C_1 \phi - \lambda_2 $ as a whole, \eqref{eq:DfPQKL} has the same form with
\eqref{eq:DfKL}.

Generally, the equation system (\ref{eq:deriv1}, \ref{eq:deriv2}, \ref{eq:deriv3})
has the following form
\begin{align}
C_2 \gamma_2(u_i) & = -\frac{p_i}{q_i}f'(\frac{p_i}{q_i}) + f(\frac{p_i}{q_i}) + \lambda_1 \label{eq:derivg1}\\
C_1 u_i & = f'( \frac{p_i}{q_i}) + \lambda_2 \label{eq:derivg2}\\
C_1 p_i + C_2 q_i \gamma'_2(u_i) &= 0 \label{eq:derivg3}
\end{align}
Actually \eqref{eq:derivg3} is redundant, which can be seen by multiplying $q_i$
in \eqref{eq:derivg1} and multiplying $p_i$ in \eqref{eq:derivg2}. Adding them together,
we can get
$C_1 p_i u_i + C_2 q_i\gamma_2 (u_i)  = \lambda_1 + \lambda_2$. Then taking the derivative about
$u_i$ yields \eqref{eq:derivg3}.

Let $t=f'(\frac{p_i}{q_i}), g= (f')^{-1}$ (inverse of $f'$), then the right hand of \eqref{eq:derivg1} has the form
$-g(t) \cdot t + f(g(t))+\lambda_1 = -f^*(t) + \lambda_1$.
Combining (\ref{eq:derivg1}, \ref{eq:derivg2}), we have
$C_2 \gamma_2(u_i) = -f^*(C_1 u_i - \lambda_2) + \lambda_1$.
The relationship of $\lambda_1 + \lambda_2 = 0$ is obtained similarly as for the special case
of KL divergence. Then
\begin{equation}
\gamma_2(t) = \frac{-\lambda_2}{C_2} - \frac{f^*(C_1 t - \lambda_2)}{C_2}
\end{equation}
and \eqref{eq:DfPQKL} is generalized to
\begin{equation}
D_{\textrm{KL}}(P, Q) = \sup_{\phi: \mathcal{X} \to R} C_1 \mathbb{E}_P(\phi) -\lambda_2  - \mathbb{E}_Q(f^*(C_1 \phi - \lambda_2))
\end{equation}
which is exactly what \eqref{eq:Df} says.
\section{$\chi^2$-divergence}
Choosing $f(x)=(x-1)^2$ whose conjugate is $f^*(y)=y+\frac{y^2}{4}$,
we get the definition of $\chi^2$-divergence as
\begin{equation}
\chi^2(P||Q) = \mathbb{E}_Q[(\frac{P}{Q}-1)^2] =\mathbb{E}_Q[(\frac{P}{Q})^2] -1
\end{equation}
Applying \eqref{eq:Df} and
making some linear transformation on the free variable, we get the
variational representation for $\chi^2$-divergence as
\begin{equation}\label{eq:chisquare_div}
\chi^2(P||Q) = \sup_{h: \mathcal{X} \to \mathbb{R}} 2\mathbb{E}_P[h(X)]
-\mathbb{E}_Q[h^2(X)] -1
\end{equation}
which is mentioned in Example 6.4 of Yihong Wu's lecture note \cite{yihong}.

We can obtain tighter representation than
\eqref{eq:chisquare_div} by optimizing over shift transformation and affine transformation (method of \cite{trans}).
For shift transformation only, we consider the optimization problem
$\max_a  2(\mathbb{E}_P[h(X)] - a)
-\mathbb{E}_Q[(h(X) - a)^2] -1$. Then $a=\mathbb{E}_Q[(h(X)]-1$, therefore
we get
\begin{equation}\label{eq:chisquare_div1}
\chi^2(P||Q) = \sup_{h: \mathcal{X} \to \mathbb{R}} 2[\mathbb{E}_P[h(X)]
- \mathbb{E}_Q[h(X)]]
-\Var_Q[h(X)]
\end{equation}

If we consider affine transformation, then the optimization problem is
$\max_{a,b}  2(b\mathbb{E}_P[h(X)] - a)
-\mathbb{E}_Q[(bh(X) - a)^2] -1$, from which we obtain
\begin{align*}
a & = \frac{\mathbb{E}_Q[h(X)]\mathbb{E}_P[h(X)] - \mathbb{E}_Q[h^2(X)]}{\Var_Q[h(X)]}  \\
b & = \frac{\mathbb{E}_P[h(X)] - \mathbb{E}_Q[h(X)]}{\Var_Q[h(X)]} 
\end{align*}
After simplification, the representation becomes
\begin{equation}\label{eq:chisquare_div2}
\chi^2(P||Q) = \sup_{h: \mathcal{X} \to \mathbb{R}}
\frac{(\mathbb{E}_P[h(X)] - \mathbb{E}_Q[h(X)])^2}{\Var_Q[h(X)]}
\end{equation}
\section{General representation form}
We define $\alpha: \mathbb{R}\to \mathbb{R}^{k_1}$
, $\beta: \mathbb{R}\to \mathbb{R}^{k_2}$ and $\Gamma: \mathbb{R}^{k_1 + k_2} \to \mathbb{R}$.
The expectation of a vector is applied in element-wise way.
For f-divergence, suppose it has the following representation:
\begin{equation}\label{eq:DfPQ}
D_f(P, Q) = \sup_{\phi: \mathcal{X}\to R} \Gamma(\mathbb{E}_P[\alpha(\phi)], \mathbb{E}_Q[\beta(\phi)])
\end{equation}
The functions $\alpha,\beta, \Gamma$ should
satisfy some conditions to make the representation valid.
For the representation \eqref{eq:chisquare_div1},
we have $k_1=1, \alpha(x) = x, k_2=2,\beta(x) = (x, x^2)$
and $\Gamma(x,y,z)=2x-y-(z-y^2)$. A general question is to investigate
under which conditions of $(\alpha,\beta, \Gamma)$, the representation \eqref{eq:DfPQ}
is valid.
\begin{thebibliography}{9}
	\bibitem{imp} Ruderman, Avraham, et al. "Tighter variational representations of f-divergences via restriction to probability measures." arXiv preprint arXiv:1206.4664 (2012).
	\bibitem{yihong} Yihong Wu, "Lecture Notes on: Information-Theoretic Methods
	for High-dimensional Statistics." (2020)
	\bibitem{trans} Birrell, Jeremiah, Markos A. Katsoulakis, and Yannis Pantazis. "Optimizing variational representations of divergences and accelerating their statistical estimation." arXiv preprint arXiv:2006.08781 (2020).
\end{thebibliography}
\end{document}







