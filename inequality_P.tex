\documentclass{article}
\usepackage{ctex}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage{bm}
\DeclareMathOperator\E{\mathbb{E}}
\DeclareMathOperator\Var{\mathrm{Var}}
\DeclareMathOperator{\Binom}{Binom}
\DeclareMathOperator{\Pois}{Pois}
\newtheorem{lemma}{引理}
\theoremstyle{definition}
\newtheorem{definition}{定义}
\newtheorem{example}{例}
\usepackage{mathtools}
\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiter{\ip}{\langle}{\rangle}
\begin{document}
\title{概念论常用不等式}
\author{zhaofeng-shu33}
\maketitle
\section{Markov 不等式}
设 $X$ 是非负型随机变量，则 $\Pr(X\geq a) \leq {1 \over a} \E[X]$
若$X$ 不一定非负，可通过$e$指数的形式运用Markov不等式：$\Pr(X\geq a) = \Pr(e^{tX} \geq e^{ta}) \leq { 1 \over \exp(ta)} \E[\exp(tX)]$
(后者被称为 Chernoff 界)

\section{Chebyshev 不等式}
$\Pr(\abs{X-\E[X]} \geq a) \leq {1 \over a^2} \Var[X]$
\section{Hoeffding 不等式}
设$X_i$ 独立，$\Pr(a_i \leq X_i \leq b_i) = 1, S_n = X_1 + \dots + X_n $则有
\begin{align}
\label{eq:Hoeffding1}\Pr( S_n - \E[S_n] \geq t) & \leq \exp \left( - \frac{2t^2}{\sum_{i=1}^n (b_i - a_i)^2} \right) \\
\label{eq:Hoeffding2}\Pr( \abs{S_n - \E[S_n] } \geq t) & \leq 2\exp \left( - \frac{2t^2}{\sum_{i=1}^n (b_i - a_i)^2} \right)
\end{align}
如果~\eqref{eq:Hoeffding1} 成立，对$X'_i= -X_i$ 应用~\eqref{eq:Hoeffding1}有
$$\Pr( S_n - \E[S_n]  \leq -t )  \leq \exp \left( - \frac{2t^2}{\sum_{i=1}^n (b_i - a_i)^2} \right)$$
因此~\eqref{eq:Hoeffding2} 成立。
为证~\eqref{eq:Hoeffding1}, 使用 Hoeffding 引理
\begin{lemma}
设$\E[X]=0, \Pr( a \leq X \leq b) = 1 $，则
\begin{equation}
\E[ \exp(\lambda X) ] \leq \exp\left({\lambda^2 (b-a)^2\over 8}\right)
\end{equation}
\end{lemma}
\begin{proof}[Hoeffding 不等式~\eqref{eq:Hoeffding1} 证明]
\begin{align*}
\Pr( S_n - \E[S_n] \geq t) & \leq \exp(-st) \E[\exp(s(S_n - \E[S_n]) ] \\
& \leq \exp(-st) \prod_{i=1}^n \E[\exp(s(X_i -\E[X_i]))] \\
& \leq \exp(-st) \prod_{i=1}^n \exp({s^2 (b_i - a_i)^2 \over 8 } ) \\
& = \exp(-st + \sum_{i=1}^n { s^2 (b_i - a_i)^2 \over 8} ) 
\end{align*}
取 $ s = {4t \over \sum_{i = 1}^n (b_i - a_i)^2}$ 即得到~\eqref{eq:Hoeffding1} 右端。
\end{proof}
\section{Other useful inequalities}
Falls $X_1, \dots, X_n$ unabhängige Bernoullißverteilte Zufallsgrößen mit
Parameter $p \in (0,1) $ sind, dann gilt
\begin{equation}\label{eq:3delta}
P(\sum_{i=1}^n X_i \geq (1+ \delta) pn ) \leq \exp(-\frac{\delta^2}{3} pn),
\delta \in [0,1], n \in \mathbb{N}
\end{equation}
\begin{proof}
Using Chernoff inequality we can get a tight upper bound
as
$$
P(\sum_{i=1}^n X_i \geq (1+ \delta) pn )  \leq \exp(-tn(1+\delta) p + n\log(1-p+pe^t)).
$$
Then use the Taylor approximation $\log(1+x) \leq x$ we have
$$
 \exp(-t(1+\delta) p + \log(1-p+pe^t)) \leq \exp(np(-1+e^t - t(1+\delta)).
$$
We choose $t$ to minimize the right hand side: $t=\log(1+\delta)$, thus getting
$$
P(\sum_{i=1}^n X_i \geq (1+ \delta) pn )  \leq \exp(np (\delta - (1+\delta)\log(1+\delta)).
$$
\end{proof}
Using the inequality $\log(1+\delta) \geq \frac{2\delta}{2+\delta} $ we
can get
\begin{align*}
\delta - (1+\delta)\log(1+\delta) \leq & \delta (1-\frac{2(1+\delta)}{2+\delta}) \\
& = -\frac{\delta^2}{2+\delta} \leq -\frac{1}{3} \delta^3 \textrm{ since } \delta \leq 1
\end{align*}
Therefore, \eqref{eq:3delta} holds.

Let $X_1, X_2, \dots, $ be i.i.d random variable, with zero mean and variance $\sigma^2$.
$S_n = \sum_{i=1}^n X_i$.
By central limit theorem:
$$
\lim_{n\to \infty} P(\frac{1}{\sqrt{n\sigma^2}} S_n \geq C) =
\int_{C}^{+\infty} \frac{e^{-x^2/2}}{\sqrt{2\pi}} dx
\textrm{ for any } C \in \mathbb{R}
$$
Let $C=\frac{x\sqrt{n}}{\sigma}, x>0$, we can estimate the probability
$P(\frac{S_n}{n} > x)$ by $\int_{\frac{x\sqrt{n}}{\sigma}}^{+\infty}  \frac{e^{-u^2/2}}{\sqrt{2\pi}}du
= \frac{\sigma}{x\sqrt{n}} \exp(-\frac{nx^2}{2\sigma^2})(1+o(1))$.

\section{Common Examples for Cramér Theorem}
For the random variable $X$,
$\varphi(t) = \mathbb{E}[e^{tX}]$ is its moment generating function.
$\log \varphi(t)$ is called cumulant generating function.
$I(x) = \sup_{t\in \mathbb{R}} (tx - \log \varphi(t))$ is called Legendre transformation of the function $\log \varphi$.
For common distributions, there $\log\varphi(t)$ and $I(x)$ are listed in the following table:
\begin{table}[!ht]
	\centering
\begin{tabular}{cccc}
	\hline  
	Distribution & pdf & $\log\varphi(t)$ & $I(x)$ \\
	\hline
	Gaussian $\mathcal{N}(0, \sigma^2)$ & $\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{x^2}{2\sigma^2}}, x\in \mathbb{R}$ & $\frac{\sigma^2 t^2}{2}$& $\frac{x^2}{2\sigma^2}$ \\
	Bernoulli $\Binom(p)$ & $p^x(1-p)^{1-x}, x\in \{0,1\}$ & $\log(pe^t + (1-p))$  & $D_{\textrm{KL}}(\Binom(x) || \Binom(p))$ \\
	Exponential $\textrm{E}(\lambda)$ & $\lambda e^{-\lambda x}, x>0$ & $\log\frac{\lambda}{\lambda - t}$ & $\lambda x - 1 - \log(\lambda x), x>0$ \\
	Poisson $\Pois(\lambda)$ & $ \frac{\lambda^k}{k!}, e^{-\lambda} k\in \mathbb{N}^+$ & $\lambda(e^t-1)$ & $\lambda - x+ x\log\frac{x}{\lambda}$ \\
	\hline
\end{tabular}
\end{table}
\section{ f-Divergence and its variational representation}
We use discrete distribution as an illustration.
Let $f:(0,\infty) \to \mathbb{R}$ be a convex function and $f(1)=0$.
Let $P,Q$ be two probability distributions on a measurable space
$(\mathcal{X}, \mathcal{F})$. The f-divergence of $Q$ from $P$
is defined as
\begin{equation}
D_f(P||Q) = \sum_{x\in \mathcal{X}} q(x) f\left(\frac{p(x)}{q(x)}\right)
\end{equation}
Based on Legendre transformation, we can give the variational
representation of $D_f(P||Q)$ as
\begin{equation}\label{eq:Df}
D_f(P||Q) = \sup_{t: \mathcal{X} \to R} \left(\mathbb{E}_P[t(X)] - \mathbb{E}_Q[f^*(t(X))]
\right)
\end{equation}
where $f^*$ is the convex conjugate of $f$.

\eqref{eq:Df} can be proved by considering
$f(\frac{p(x)}{q(x)}) = \sup_{t=t(x)} [t\frac{p(x)}{q(x)} - f^*(t)] $
for each $x\in \mathcal{X}$. Then summing up $\sum_{x\in\mathcal{X}} f(\frac{p(x)}{q(x)}) q(x)$
we can get \eqref{eq:Df}.

Choosing $f(x)=x\log x, f^*(t)=e^{t-1}$, the f-divergence reduces to KL-divergence and
\eqref{eq:Df} becomes
\begin{equation}\label{eq:DfKL}
D(P||Q) = \sup_{t: \mathcal{X} \to R} \mathbb{E}_P[t(X)] - \mathbb{E}_Q[\exp(t(X)-1)]
\end{equation}
A tighter variational representation of KL divergence is given 
by Donsker-Varadhan theorem as:
\begin{equation}\label{eq:DfKL_tight}
D(P||Q) = \sup_{t: \mathcal{X} \to R} \mathbb{E}_P[t(X)] - \log\mathbb{E}_Q[\exp(t(X))]
\end{equation}
using $\log x \leq x-1$ we can show that
$\mathbb{E}_P[t(X)] - \log\mathbb{E}_Q[\exp(t(X))]
\geq \mathbb{E}_P[t(X)] - \mathbb{E}_Q[\exp(t(X)-1)]$.

Compared with \eqref{eq:Df}
, a tighter representation for f-divergence is given in Theorem 1 of
\cite{imp}.
\begin{equation}\label{eq:Dft}
D_f(P||Q) = \sup_{t: \mathcal{X} \to \mathbb{R}} \mathbb{E}_P[t(X)] -  (\mathbbm{1}^R_{f,Q})^* (t)
\end{equation}
The joint convex conjugate of $f,Q$ is defined as $g(t) := (\mathbbm{1}^R_{f,Q})^*(t) $
\begin{equation}
g(t) = \sup_{r, \mathbb{E}_Q[r(X)] = 1} \ip{t,r}_Q - \mathbb{E}_Q[f(r(X))] 
\textrm{ for } t: \mathcal{X} \to \mathbb{R}
\end{equation}
The inner product $\ip{t,r}_Q = \sum_{x\in \mathcal{X}} t(x)r(x)Q(x)$.
Choosing $f(x)=x\log x$, we can show that
\eqref{eq:Dft} reduces to
\eqref{eq:DfKL_tight} (using Lagrange multiplier).
\section{New variational representation of KL divergence}
Xiangxiang Xu derives the following expression of KL divergence.
\begin{equation}
D(P||Q) = \sup_{t: \mathcal{X} \to R} \mathbb{E}_P[t(X)] - \mathbb{E}_Q[\exp(t(X))] +1
+\frac{1}{2}(\mathbb{E}_Q[\exp(t(X))]-1)^2
\end{equation}
\begin{thebibliography}{9}
	\bibitem{imp} Ruderman, Avraham, et al. "Tighter variational representations of f-divergences via restriction to probability measures." arXiv preprint arXiv:1206.4664 (2012).
\end{thebibliography}
\end{document}







