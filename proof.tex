\documentclass{article}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{algorithm,algorithmic}
\usepackage{url}
\usepackage{amssymb}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{corollary}{Corollary}
\DeclareMathOperator{\E}{\mathbb{E}}
\title{Proofs for semi-supervised SBM}
\begin{document}
\maketitle
\begin{lemma}
Let $Z_{i2}$ follows Bernoulli distribution with parameter $q=\frac{\beta \log n }{n}$.
Let $Z_{i1} \sim Bern(p)$ with $p=\frac{\alpha \log n }{n}$. $Z_{i2}$ are i.i.d. for $i=1,\dots, n$.
The same is true for $Z_{i1}$. $\beta < \alpha$.
We then have $P(\sum_{i=1}^{n} (Z_{i2} - Z_{i1}) \geq  D \log n) \leq n^{-C}$
where $C=\frac{3(\alpha - \beta + D)^2}{2(4\alpha + 2\beta + D)} + O(\log n /n)$.
\end{lemma}
\begin{proof}
	Let $X_i = Z_{i2} - \frac{\beta \log n }{n}, X_{i+n} = \frac{\alpha \log n }{n} - Z_{i1}$.
	$X_i$ are zero means and $|X_i| \leq 1 + O(\frac{\log n}{n})$.
	Then $\sum_{i=1}^{n} (Z_{i2} - Z_{i1}) = \sum_{i=1}^{2n} X_{i} - (\alpha - \beta)\log n $.
	Then $\sum_{i=1}^{n} (Z_{i2} - Z_{i1}) \geq D \log n \iff \sum_{i=1}^{2n} X_{i} \geq (D + \alpha - \beta)\log n$.
	$\sum_{i=1}^{2n} E[X_i^2] = (\alpha + \beta)\log n + O(\frac{\log^2 n}{n})$.
	By Bernstein's inequality, we have
	$$
	P(\sum_{i=1}^{2n} X_{i}  \geq t) \leq \exp(-\frac{1/2 t^2}{\sum_{i=1}^{2n} E[X_i^2] + \frac{1}{3}(1+O(\log n /n))t})
	$$
	Let $t=(D + \alpha - \beta)\log n$.
	\begin{align}
	\frac{1/2 t^2}{\sum_{i=1}^{2n} E[X_i^2] + \frac{1}{3}t} & 
	= \frac{1/2(D + \alpha - \beta)^2\log^2 n}{(\alpha + \beta)\log n + O(\frac{\log^2 n}{n}) + 1/3(D + \alpha - \beta)\log n } \\
	 & =\frac{3/2(D + \alpha - \beta)^2\log n}{O(\frac{\log n}{n}) + (D + 4\alpha - 2\beta) }
	\end{align}
\end{proof}
Notation: $f(n)\dot{=} g(n) \iff \lim_{n\to \infty} \frac{1}{n} \log \frac{f(n)}{g(n)} = 1$.
\begin{lemma}\label{lem:coupled}
	Suppose $x_i$ is i.i.d. sampled from $p_1$ and $z_i$ is i.i.d. sampled from $Bern(p)$.
	$p > q$
Let $A := \frac{1}{n}\sum_{i=1}^n \log \frac{p_0(x_i)}{p_1(x_i)} \geq \frac{1}{n} \sum_{i=1}(z_i \log\frac{p}{q} +(1-z_i) \frac{\log(1-p)}{\log(1-q)}))$.
Show that $P(A) \dot{=} \exp(-n C)$.
\end{lemma}
\begin{proof}
	To apply Sanov's theorem, we treat $(x_i, z_i)$ jointly sampled from $\mathcal{X} \times \mathcal{Z}$
	and the constraint is 
	\begin{equation}\label{eq:fxz}	
	\frac{1}{n} \sum_{i=1}^n f(x_i, z_i) \geq 0
	\end{equation}	
	where
	$$
	f(x, z) = \log\frac{p_0(x)}{p_1(x)} + z \log\frac{q}{p} + (1-z) \log \frac{1-q}{1-p}
	$$
	Then by Sanov's theorem, we have $C = D(p^*(x,z)|| p_1(x)Bern(p))$
	where $p^*(x,z)$ is the minimizer for $D(p(x,z)|| p_1(x)Bern(p))$ subjected to
	\eqref{eq:fxz}.
	We can rewrite \eqref{eq:fxz} in probability form:
	\begin{equation}\label{eq:probxz}
	\sum_{x\in \mathcal{X}} p(x) \log\frac{p_0(x)}{p_1(x)} + \sum_{z \in \{0,1\}} p(z) \log\frac{p_{Bq}(z)}{p_{Bp}(z)} \geq 0
	\end{equation}
	where $p(x), p(z)$ are the marginal probability mass function.
	Using Lagrange's method, we can get
	\begin{equation}
	p^*(x,z) = C_1 p_0^{\lambda}(x)p_1^{1-\lambda}(x) \cdot C_2 p_{Bq}^{\lambda}(z)p_{Bp}^{1-\lambda}(z)
	\end{equation}
	where $\frac{1}{C_1} = \sum_{x\in\mathcal{X}}p_0^{\lambda}(x)p_1^{1-\lambda}(x) $
	and $\frac{1}{C_2} = \sum_{z\in \{0,1\}} p_{Bq}^{\lambda}(z)p_{Bp}^{1-\lambda}(z)$
	The parameter $\lambda$ is solved by letting the left hand of \eqref{eq:probxz} takes zero.
	That is:
	\begin{equation}\label{eq:constraint}
	C_1 \sum_{x\in \mathcal{X}} p_0^{\lambda}(x)p_1^{1-\lambda}(x)\log\frac{p_0(x)}{p_1(x)} + C_2[p^{1-\lambda}q^{\lambda}\log\frac{q}{p} +
	(1-p)^{1-\lambda}(1-q)^{\lambda}\log\frac{1-q}{1-p} ] = 0
	\end{equation}
	On the other hand,
	\begin{align*}
	C(k) & = D(p(x)|| p_1(x)) + D(p(z) || Bern(p)) \\
	& = C_1\sum_{x\in \mathcal{X}} p_0^{\lambda}(x)p_1^{1-\lambda}(x)\log\frac{C_1  p_0^{\lambda}(x)p_1^{1-\lambda}(x)}{p_1(x)}  \\
	& + C_2[p^{1-\lambda}q^{\lambda}\log\frac{C_2 p^{1-\lambda}q^{\lambda}}{p} +
	(1-p)^{1-\lambda}(1-q)^{\lambda}\log\frac{C_2(1-p)^{1-\lambda}(1-q)^{\lambda}}{(1-p)} ] 
	\end{align*}
	Using \eqref{eq:constraint}, we can simplify $C$ as
	\begin{equation}
	C  =  \log  C_1 + \log C_2
	\end{equation}
	Using $p_0^{\lambda} p_1^{1-\lambda} \leq \lambda p_0 + (1-\lambda) p_1$ we can get $C_1 > 1, C_2 > 1$.

\end{proof}
\begin{corollary}\label{cor:xz}
	Let $p_0 \sim Bern(p), p_1 \sim Bern(q)$ in Lemma \ref{lem:coupled}. Then the region $A$ can be simplified as $\sum_{i=1}^n (x_i - z_i) \geq 0$,
	where $x_i \in Bern(q), z_i \in Bern(p)$ and both are i.i.d. sampled. We can compute $\lambda = \frac{1}{2}$ and the error exponent
	$C = -2\log(\sqrt{pq} + \sqrt{(1-p)(1-q)})$. Below we give another way to prove that $P(A) \dot{=} \exp(-nC)$.
\end{corollary}
\begin{proof}
	We use Chernoff inequality to give a tight upper bound.
	$P(\sum_{i=1}^n (x_i - z_i) \geq 0) \leq \E[\exp(\beta \sum_{i=1}^n (x_i - z_i))]$ where $\beta > 0$.
	$\E[\exp(\beta \sum_{i=1}^n (x_i - z_i))] = (1-q+q e^{\beta})^n(1-p+p e^{-\beta})^n$. Let
	$g(\beta)  = (1-q+q e^{\beta})(1-p+p e^{-\beta})$. $g(0) = 1$ and when $\beta$ is very small,
	$g(\beta) = 1 + (q-p) \beta + o(\beta)$. Therefore, we can choose a proper $\beta$ such that $g(\beta) < 1$.
	Since $\beta$ can take any positive value, now we compute the minimum value of $g(\beta)$ instead.
	the optimal $\beta^*$ satisfies $e^{2\beta} = \frac{(1-q)p}{(1-p)q}$. Now we compute $C' = -\log g(\beta^*) = C$.
	Therefore, $P(A) \leq \exp(-nC)$.
	
	To simplify our discussion, let $X_n = \sum_{i=1}^n X_i, Z_n = \sum_{i=1}^n Z_i$. We have $X_n \sim Binomial(n, q),
	Z_n \sim Binomial(n, p)$ and $X_n, Z_n$ are independent.
	
	On the other hand $P(A) = \sum_{j=1}^n\sum_{k=0}^j P(X_n = j)P(Z_n = k)$.
	We choose a proper $j,k$ such that $P(X_n = j)P(Z_n = k)$ is maximized.
	Since we observe $P(X_n - Z_n = r)$ decreases as $r$ increase ($r\geq 0$).
	The maximum value is taken when $j=k$. On the other hand,
	we should choose:
	$$
	k = n \frac{\sqrt{pq}}{\sqrt{pq} + \sqrt{(1-p)(1-q)}}
	$$
	We have
$$
		P(X_n = k)P(Z_n = k) = \binom{n}{k}^2 p^k (1-p)^{n-k}q^k(1-q)^{n-k}
$$
We will use the inequality $\binom{n}{k} \geq \frac{1}{n+1}\exp(nH(k/n))$.
Then
$$
\log P(X_n = k)P(Z_n = k) \geq -2\log(n+1) + 2n \log(\sqrt{pq} + \sqrt{(1-p)(1-q)})
$$
Therefore, $\log P(A) \geq \log P(X_n = k)P(Z_n = k) \geq -2\log(n+1) + 2n \log(\sqrt{pq} + \sqrt{(1-p)(1-q)})$.
That is $P(A) \geq \frac{1}{(n+1)^2}\exp(-nC)$.
The polynomial term is neglectable compared with the exponential term.
\end{proof}
\begin{corollary}\label{cor:decrease}
	Now we modify the condition in \ref{lem:coupled} as follows:
	$$
	A := \frac{1}{n}\sum_{i=1}^n \log \frac{p_0(x_i)}{p_1(x_i)} \geq \frac{1}{n} \sum_{i=1}(z_i \log\frac{p}{q} +(1-z_i) \frac{\log(1-p)}{\log(1-q)}) + \epsilon\frac{\log n }{n} [\log \frac{p}{q} - \log \frac{1-p}{1-q}]
	$$
	Take the same assumption in Corollary \ref{cor:xz} that
	$p_0 \sim Bern(p), p_1 \sim Bern(q)$, we now have $\sum_{i=1}^n (x_i - z_i) \geq \epsilon \log n$.
	This problem is interesting when
	$ p = \frac{a \log n }{n} $ and $ q = \frac{b \log n }{n}$. Follow the same procedure as
	in the proof of Lemma \ref{lem:coupled}. The conclusion is that $P(A) \dot{=} n^{-g(a,b,\epsilon)}$.
	where $g(a,b,\epsilon)$ is defined as:
	\begin{equation}\label{eq:gab}
	g(a,b,\epsilon) = a + b - \sqrt{\epsilon^2 + 4ab} + \epsilon \log \frac{\epsilon + \sqrt{\epsilon^2 + 4ab}}{2b}
	\end{equation}
	Notice that Equation \eqref{eq:gab} is the same as that of Equation \eqref{eq:galphabeta}.
	\begin{remark}
		we have $g'(\epsilon) = \log \frac{\epsilon + \sqrt{\epsilon^2 + 4ab}}{2b}$ and $g''(\epsilon ) = \frac{1}{\sqrt{4ab + \epsilon^2}}$.
		\end{remark}
	\begin{proof}
	Our $\lambda$ is no longer equal to $\frac{1}{2}$ but is the solution to the following equation, modified directly from Equation \eqref{eq:constraint}:
	\begin{equation}\label{eq:Clambda}
	C_1 \sum_{x\in \mathcal{X}} p_0^{\lambda}(x)p_1^{1-\lambda}(x)\log\frac{p_0(x)}{p_1(x)} + C_2 \sum_{z\in \mathcal{X}} p_0^{1-\lambda}(z)p_1^{\lambda}(z)\log\frac{p_1(z)}{p_0(z)}= \epsilon\frac{\log n }{n} [\log \frac{p}{q} - \log \frac{1-p}{1-q}]
	\end{equation}
	And $C= \log C_1 + \log C_2 + \lambda \epsilon\frac{\log n }{n} [\log \frac{p}{q} - \log \frac{1-p}{1-q}]$
	Using the Taylor expansion we have
	$$
	C = (a + b - a^{\lambda}b^{1-\lambda} - a^{1-\lambda}b^{\lambda} + \lambda \epsilon \log\frac{a}{b}) \frac{\log n }{n}
	+ o(\frac{\log n}{n})
	$$
	The $\lambda$ is solved by computing the dominant term on both sides of Equation \eqref{eq:Clambda} and we get
	\begin{equation}
	 a^{\lambda}b^{1-\lambda} - a^{1-\lambda}b^{\lambda} = \epsilon
	\end{equation}
	Solving the above equation we can get the dominant term of $\lambda^*$, as $n\to \infty$, $\lambda_n \to \lambda^*$.
	we have:
	\begin{equation}
	\lambda^*= \frac{
				\log \frac{ 
			\epsilon + \sqrt{\epsilon^2 + 4 ab }	
			} { 2b}
	}
	{\log \frac{a}{b}}
	\end{equation}
	which is larger than $\frac{1}{2}$. we also get the necessary condition by letting $\lambda > 0 \rightarrow \epsilon > b - a$.  Also by letting $\lambda < 1$ we have $\lambda < a - b$.
	\end{proof}
\end{corollary}
\begin{lemma}\label{lem:nk}
	$\binom{n}{k} < (\frac{ne}{k})^k$
\end{lemma}
\begin{proof}
	$\binom{n}{k} = \frac{n!}{(n-k)!k!} < \frac{n^k}{k!}$.
	Since $e^z = \sum_{k=0}^{\infty} \frac{z^k}{k!} > \frac{z^k}{k!}$,
	let $z=k$ we have $e^k > \frac{k^k}{k!}$.
	Therefore, $\frac{1}{k!} < \frac{e^k}{k^k}$.
	As a result, $\binom{n}{k}  < \frac{n^k}{k!} < \frac{n^k e^k}{k^k}
	= (\frac{ne}{k})^k$
\end{proof}
\begin{theorem}\label{thm:2pq}
	For a $\textrm{SBM}(n,2,p, q)$ with $p,q$ as constant, $p > q$. The
	exact recovery error decreases in $\exp(-n (C + o(1)))$
	where $C = -2\log(\sqrt{pq} + \sqrt{(1-p)(1-q)})$.
\end{theorem}
\begin{proof}
	We use maximum likelihood estimation method, let $F$ be the event of the maximum likelihood estimator not coinciding with the ground truth. 
    
    We use $P_n^{(k)}$ to denote the event when there are $k$ pairs wrongly classified. Due to the symmetric property of the problem, $1\leq k \leq \frac{n}{4}$.
    $P_n^{(k)}$ happens when the probability of one $k$ mis-classified configuration has larger probability than that of the ground truth. 
	
	Therefore, we consider one configuration of $k=1$. Let event $A$ be the ground truth and $A_1$ be our chosen event:
	\begin{align*}
	A: & y_1 = 1, y_2 = 0, y_3 = 1, y_4 = 0, \dots, y_{n-1} = 1, y_n = 0 \\
	A_1: & y_1 = 0, y_2 = 1, y_3 = 1, y_4 = 0, \dots, y_{n-1} = 1, y_n = 0 \\	
	\end{align*}
	Then if $P(A) < P(A_1)$, event $F$ will happen. Therefore, we use $P(P(A) < P(A_1))$ as the lower bound of $P(F)$.
	By simplification we get $P(\sum_{i=1}^{n-2} (x_i - z_i) \geq 0) \dot{=} \exp (-nC)$ where $x_i, z_i$ are the same with Corollary \ref{cor:xz}.
	
	For the upper bound, we next show that 
	\begin{equation}
	P(F) \leq \frac{en^2}{4} \exp(-nC).
	\end{equation}
	where $e$ is the natural logarithm number.
	
	When $F$ happens, then $P(A)$ is not the maximum value, there must exists one configuration $P(A') > P(A)$, by the union bound we have:
	\begin{equation}\label{eq:outsidelemmaboundpf5_IT_ub}
	P(F) \leq \sum_{A'} P(P(A') > P(A))
	\end{equation}
	We can classify the right hand by grouping according to the index $k$. That is
	\begin{equation}\label{eq:with14epsilon_14}
	\sum_{A'} P(P(A') > P(A)) = \sum_{k=1}^{n/4} P_{n}^{(k)}
	\end{equation}
	where $P_{n}^{(k)} = \binom{n/2}{k}^2 P(P(A_k) > P(A))$ and $A_k$ is one configuration with $k$ pairs mis-classified.
	Similar to the case $k=1$ we can compute $P(P(A_k) > P(A)) = P(\sum_{i=1}^{k(n-2k)} (x_i - z_i) \geq 0)\dot{=} \exp(-k(n-2k)C)$.
	
	Combining (\ref{eq:outsidelemmaboundpf5_IT_ub}) and (\ref{eq:with14epsilon_14}), we have
	\begin{equation}
	P(F) \leq \sum_{k=1}^{n/4} \binom{n/2}{k}^2 \exp(-k(n-2k)C)
	\end{equation}
	Using the inequality from Lemma \ref{lem:nk}, we have
	\begin{equation}\label{eq:Ff}
		P(F) \leq \sum_{k=1}^{n/4} \exp(-nf(k))
	\end{equation}
	where $f(k)= \frac{2k}{n}\log\frac{2k}{ne} + k(1-\frac{2k}{n})C$.
	By computing $f'(x)= \frac{2}{n} \log \frac{2x}{n} + C - \frac{4Cx}{n}$, $1\leq x \leq \frac{n}{4}$.
	$f'(1) > 0 , f'(\frac{n}{4}) < 0$. Therefore, $f(x)$ increases first and decreases in the interval $[1, \frac{n}{4}]$.
	Comparing $f(1)$ and $f(\frac{n}{4})$ we have $f(1) < f(\frac{n}{4})$. Therefore, $f(k) \geq f(1)$ for $1\leq k \leq \frac{n}{4}$.
	\begin{equation}
	P(F) \leq \frac{n}{4}\exp(-nf(1)) = \exp(-n (C+g(n)))
	\end{equation}
	where $g(n) = -\frac{1}{n}(\log (n/4) + 2 C - 2 \log (2/ne)) = o(1)$
\end{proof}
\begin{theorem}
	In $\textrm{SBM}(n, 2, p, q)$ where $p,q$ are constant, each node has further $n$ observations based on its label.
	That is $ x_{ij} \sim p_{y_i}$ where $y_i = 0 $ or $y_i = 1$. $D(p_0 || p_1) > 0$. Then generally we will have
	the exact recovery error decreases in $\exp(-nC')$. where $C'$ is a constant irreverent with $n$.
\end{theorem}
\begin{proof}
	The proof is similar with that of Theorem \ref{thm:2pq} and we have
	\begin{align}
	C' &= C_1 + C_2  \\
	C_1 &= - 2 \log \sum_{x\in \mathcal{X}} \sqrt{p_0(x)p_1(x)} \\
	C_2 &= -2 \log(\sqrt{pq} + \sqrt{(1-p)(1-q)}) 
	\end{align}
	In short, the error exponent comes from the case when one pair is wrongly classified.
	The event $P(A) < P(A_1)$ can be written as:
	\begin{align*}
	&T_1 = \sum_{x \in \mathcal{X}} p_{X_1}(x) \log \frac{p_0(x)}{p_1(x)}
	+ \sum_{x \in \mathcal{X}} p_{X_2}(x) \log \frac{p_1(x)}{p_0(x)}
	+ \sum_{z \in \{0,1\}} p_{Z_1}(x) \log \frac{p_{B_q}(x)}{p_{B_p}(x)}\\
	&
	+ \sum_{z \in \{0,1\}} p_{Z_2}(x) \log \frac{p_{B_p}(x)}{p_{B_q}(x)} \geq 0
	\end{align*}
	we use Sanov' theorem, and the error exponent is the minimum value of
	$$
	T_2 = D(p_{X_1} || p_1) + D(p_{X_2} || p_0) + D(p_{Z_1} || p_{B_p}) + D(p_{Z_2} || p_{B_q})
	$$
	Using Lagrange's method to minimize $T_2 + \lambda T_1$, we have
	\begin{align*}
	p_{X_1}(x) &= \frac{p_1^{1-\lambda}(x)p_0^{\lambda}(x)}{\sum_{x\in\mathcal{X}}p_1^{1-\lambda}(x)p_0^{\lambda}(x)} \\
	p_{X_2}(x) &= \frac{p_0^{1-\lambda}(x)p_1^{\lambda}(x)}{\sum_{x\in\mathcal{X}}p_0^{1-\lambda}(x)p_1^{\lambda}(x)} \\	
	p_{Z_1}(z) &= \frac{p_{B_p}^{1-\lambda}(x)p_{B_q}^{\lambda}(z)}{\sum_{z\in\{0,1\} }p_{B_p}^{1-\lambda}(z)p_{B_q}^{\lambda}(z)} \\
	p_{Z_2}(z) &= \frac{p_{B_q}^{1-\lambda}(x)p_{B_p}^{\lambda}(z)}{\sum_{z\in\{0,1\} }p_{B_q}^{1-\lambda}(z)p_{B_p}^{\lambda}(z)}	
	\end{align*}
	Taking $\lambda = \frac{1}{2}$ we have $T_1 = 0$ and $T_2 = C'$.
	For $P(P(A_k) > P(A))$ we have it equal to $\exp(-nk C_1 - k(n-2k)C_2)$.
	Similar to the discussion of \eqref{eq:Ff}, we have
	$P(F) \leq \sum_{k=1}^{n/4}P(P(A_1) > P(A)) \dot{=} \exp(-nC_1 - nC_2)$.
\end{proof}
\begin{lemma}
	Suppose $Z \sim Binomial(n, \frac{b\log n}{n}), X\sim Binomial(n, \frac{a\log n}{n})$.
	For $ t > b - a$, show that
	\begin{equation}
	P(Z - X \geq \epsilon \log n) \leq \exp(-\log n \cdot ( g(a, b, \epsilon) + O(\frac{\log n}{n})))
	\end{equation}
	where
	\begin{equation}\label{eq:galphabeta}
	g(\alpha,\beta,\epsilon)= (\alpha+\beta)-\epsilon \log(\beta) - 2\sqrt{\left( \frac{\epsilon}{2}\right)^2 +\alpha \beta} +\frac{\epsilon}{2} \log \left( \alpha \beta \frac{\sqrt{(\epsilon/2)^2 +\alpha \beta} +\epsilon/2}{\sqrt{(\epsilon/2)^2 +\alpha \beta} -\epsilon/2} \right)
	\end{equation}
\end{lemma}
\begin{proof}
	The proof is similar with that of Corollary \ref{cor:xz}.
	Use Chernoff Inequality to proof the upper bound. For the lower bound, we consider
	\begin{align*}
	P(Z - X \geq \epsilon \log n) & \geq P(Z - X = \epsilon \log n) \\
	& = \sum_{\tau \log n = 0}^{n - \epsilon \log n} P(X = \tau \log n) P(Z = (\tau + \epsilon) \log n)
	\end{align*}
	Let
	\begin{align*}
	T(\tau) &= P(X = \tau \log n) P(Z = (\tau + \epsilon) \log n) \\ 
	\log T(\tau) & = \log \binom{n}{\tau \log n} +  \log \binom{n}{(\tau + \epsilon) \log n}
    +  \tau \log n \log p + (\tau + \epsilon) \log n \log q \\
    &+ (n - \tau \log n) \log (1 - p)
    + (n - (\tau + \epsilon) \log n ) \log (1 - q )
	\end{align*}
	We need to choose a $\tau=\tau^*$ such that $T(\tau)$ is maximized. Then
	$ P(Z - X \geq \epsilon \log n) \geq T(\tau^*)$.
	We need a good estimate of $\binom{n}{\tau \log n}$ to write an asymptotic expression of $\log T(\tau)$.
	The way to estimate $\log T(\tau)$ and compute its maximum value 
    is covered in the proof Lemma 7 of \cite{abbe}.
\end{proof}
\begin{lemma}
	Suppose $m > n, Z \sim Binomial(m, \frac{b\log n}{n}), X\sim Binomial(m, \frac{a\log n}{n})$.
	For $ t > \frac{m}{n}(b - a)$, show that
	\begin{equation}
	P(Z - X \geq t \log n) \leq \exp(-\log n \frac{m}{n}\cdot ( g(a, b, \frac{n}{m}t) + O(\frac{\log n}{n})))
	\end{equation}
	where $g(a,b,\epsilon)$ is defined in \eqref{eq:gab}.
\end{lemma}
\begin{proof}
	Similar with the proof of Proposition 5 in \cite{yemin}.
\end{proof}
\begin{lemma}
	For an ER graph with parameter $p=\frac{c\log n}{n}$. When $c>1$, the graph is connected with probability
	$1-o(1)$. When $c<1$, the graph is connected with probability $o(1)$. 
\end{lemma}
\begin{proof}
	The problem is closely related with the probability $P(\sum_{i=1}^n X_i > 1)$ where $X_1, \dots, X_n$
	i.i.d. $\sim Bern(p)$.
	When $c > 1$ by large deviation theory $P(\sum_{i=1}^n X_i = 0)$ decreases in polynomial rate with respect with $n$. To be more specific, we consider the number of node which is disconnected with others.
	To be continued.
\end{proof}
\begin{lemma}\label{lem:ab2}
	If $\sqrt{a} - \sqrt{b} < \sqrt{2}$, then exact recovery for $SSBM(n, 2, \frac{a \log n}{n}, \frac{b \log n}{n})$
	is impossible.
\end{lemma}
\begin{proof}
	We follow the main idea of Abbe's proof in \cite{abbe} but we give a stronger version.
	We assume $i \in A, j \in B$ where $A$ and $B$ denote two communities.
	We want to show that $P(F) = 1 - o(1)$ by considering $P(F_1)$.
	$F_1: \exists i,j$ such that $E(i, B \backslash \{j\}) + E(j, A \backslash \{i\}) \geq E(i, A \backslash \{i\}) + E(j, B \backslash \{j\})$.
	Then $P(F) \geq P(F_1)$.
	
	Let $F_1(i,j)$ represent the event when $i,j$ are chosen. Then $F_1 = \cup_{i,j}F_1(i,j)$.
	
	Consider $F_1^c = \cap_{i\in A, j\in B} F_1^c(i,j)$. (or $1\leq i \leq n/2, n/2+1 \leq j \leq n$)
	
	Let $H_1\subseteq A, H_2 \subseteq B$ such that $|H_1| = |H_2| = \frac{n}{\log^3 n}$.
	
	To upper bound $P(F_1^c)$ we restrict $i \in H_1, j \in H_2$ and consider
	$G_1(i,j): E(i, B \backslash \{j\}) + E(j, A \backslash \{i\}) \geq E(i, A \backslash H_1) + E(j, B \backslash H_2) + 2\delta(n)$
	where $\delta(n) = \frac{\log n}{\log \log n}$.
	The advantage of considering $G_1(i,j)$ is that $G_1(i,j)$ and $G_1(i', j')$ are independent.
	Let the event $\Delta_1$ represents all nodes in $H_1$ are connected to less than $\delta(n)$ other nodes in $H_1$.
	$\Delta_2$ is defined similarly.
	
	Then we have $F_1^c \Rightarrow \Delta_1^c \cup \Delta_2^c \cup \cap_{i\in A, j\in B}G^c_1(i,j)$
	
	Therefore, we have $P(F_1^c) \leq P(\Delta_1^c) + P(\Delta_2^c) + P(\cap_{i\in A, j\in B}G^c_1(i,j))$.
	We can show that $P(\Delta_1^c) = o(1)$ from Lemma 10 of \cite{abbe}.
	Similar to the analysis in Lemma 4 of \cite{abbe}, we have $P(G_1(i,j)) > n^{-(\alpha - \beta)^2}$
	and $P(\cap_{i\in A, j\in B}G^c_1(i,j)) = (1 - P(G_1(i,j)))^{|H_1|^2} \sim \exp(-\frac{2 - (\sqrt{\alpha} - \sqrt{\beta})^2}{\log^6 n}) \to 0$.
	We get the conclusion that $P(F_1^c) \to 0$ and $P(F) \to 1$.
	
\end{proof}
\begin{lemma}
	Let $m=\gamma\log n$,
	If $(\sqrt{a} - \sqrt{b})^2 - 2\gamma\log\sum_{x\in\mathcal{X}} \sqrt{p_0(x)p_1(x)} < 2$, then exact recovery for the mixed model with $SSBM(n, m, 2, p_0, p_1, \frac{a \log n}{n}, \frac{b \log n}{n})$
	is impossible.
\end{lemma}
\begin{proof}
	Let $C = - 2\gamma\log\sum_{x\in\mathcal{X}} \sqrt{p_0(x)p_1(x)}$ which is positive.
	The proof is similar with Lemma \ref{lem:ab2} with some modification that we need to add $\sum_{s=1}^m \log \frac{p_0(x^{(i)}_s)}{p_1(x_s^{(i)})}
	+ \sum_{s=1}^m \log \frac{p_1(x^{(i)}_s)}{p_0(x_s^{(i)})}$ to the left hand of the event $F_1(i,j), G_1(i,j)$. ($x_s^{(i)}$ is the sample for the $i$-th node)
	Also we need
	$P(G_1(i,j)) > n^{-(\sqrt{\alpha} - \sqrt{\beta})^2 + C)}$.
\end{proof}
\begin{lemma}
	If $\sqrt{a} - \sqrt{b} > \sqrt{2}$ and we consider SSBM $(n,2, \frac{a\log n}{n}, \frac{b \log n}{n})$, for ML algorithms, the exact recovery error rate is $O( n^{2-(\sqrt{a} - \sqrt{b})^2})$.
\end{lemma}
\begin{proof}
	In Abbe's original proof (Section 6 of \cite{abbe}), he gave an upper bound of the error rate but that bound is not tight.
	We enhance his results by more careful analysis to show that $P(F) \leq cn^{2\epsilon}$.
	We start with the following inequality, which is Equation 14 in \cite{abbe}.
	\begin{equation}
	P(F) \leq \sum_{k=1}^{n/4} \binom{n/2}{k}^2 \exp \Big(-\frac{\log n}{n}4k(n/2-k)(\frac{a+b}{2} - \sqrt{ab})  \Big)
	\end{equation}
	Let $\frac{a+b}{2} - \sqrt{ab} = 1 + \epsilon$ where $\epsilon > 0$, $f(k) = -2\log(2k) + 4\frac{k}{n} \log n - (\frac{1}{2} - \frac{k}{n}) 4\epsilon \log n + 2$,
	then we have
	$P(F)\leq \sum_{k=1}^{\sqrt{n}} \exp(kf(k)) + I_2$. where
	\begin{equation}
	I_2 = \sum_{k=\sqrt{n}}^{n/4}\binom{n/2}{k}^2 \exp \Big(-\frac{\log n}{n}4k(n/2-k)(\frac{a+b}{2} - \sqrt{ab})  \Big)
	\end{equation}
	$f'(x) = \frac{-2}{x} + \frac{4(1+\epsilon)\log n}{n}$, for $n$ sufficiently large, we have $f'(1) < 0$, $f'(\sqrt{n})<0$ and $f(x)$ takes maximal value
	at $f(1)$. Therefore, $f(k)\leq f(1)$ for $1\leq k \leq \sqrt{n}$. Then,
	$\sum_{k=1}^{\sqrt{n}} \exp(kf(k)) \leq \frac{\exp(f(1))}{1-\exp(f(1))}$. $\exp(f(1))\sim C n^{-2\epsilon}$ where $C$ is a constant.
	
	Now we want to estimate $I_2$ by applying Stirling's formula.
	\begin{align*}
	I_2 \leq & C'\sum_{k=\sqrt{n}}^{n/4} \frac{n}{4\pi k (n/2-k)} \frac{(n/2)^{n}}{k^{2k} (n/2-k)^{n-2k}} \exp(-\frac{\log n}{n}4k(n/2-k)(1+\epsilon))\\
	= & C'\sum_{k=\sqrt{n}}^{n/4} \exp(kf_2(k))
	\end{align*}
	where 
	\begin{align*}
	f_2(x) &= \frac{n}{x} \log \frac{n}{2} - 2\log x \\
	&- (\frac{n+1}{x} - 2)\log(n/2-x) + \frac{1}{x} \log n - \frac{1}{x} \log(4\pi x)
	- (\log n)(2+2\epsilon) + \frac{x}{n}(\log n)(4+4\epsilon) \\
	f'_2(x) & = -\frac{n}{2x^2}\log(n/2) - \frac{1}{x} -\frac{\log n}{2x^2} \\
	&+ \frac{n+1}{2x^2}\log(n/2-x) + (\frac{n+1}{2x}-1)\frac{1}{n/2-x} +\frac{\log (4\pi x)}{2x^2}-\frac{1}{2x^2} + \frac{\log n}{n}(4+4\epsilon)
	\end{align*}
	We consider two regimes, $k=\Theta(n)$ and $k=o(n)$, for the first regime, the dominant term is $\frac{\log n}{n}(4+4\epsilon)$ and we have
	$f'_2(x) > 0$, for the second regime, we use the limit $k/n = o(1)$ to simplify 
	$$
	f'_2(x) \sim -\frac{1}{x} -\frac{1}{nx} + \frac{\log(4\pi x)}{2x^2} - \frac{1+\log(2)}{2x^2} + \frac{\log n}{n}(4+4\epsilon)
	$$
	which is negative for $k=o(\frac{n}{\log n})$.
	Therefore, $f_2(x)$ takes minimum value within $[\sqrt{n}, n/4]$.
	
	For the lower bound part, we consider the event $A_1: $ ML algorithm fails only at one pair. Then $P(F) \geq P(A_1)$.
	$A_1$ can further be decomposed into a serial of event $A_{ij}:$ AL algorithm fails only at $(i,j)$ pair. That is:
	$P(A_1) = \sum_{i,j} P(A_{ij}) = (n/2)^2 P(A_{12})$ since $A_{ij}$ are mutual exclusive.
	We only need to show $P(A_{12})$ is lower bounded by $n^{-(\sqrt{a}-\sqrt{b})^2}$.
	
	We consider the optimization problem:
	$f(\sigma) = \sum_{i,j} A_{ij}\delta_{\sigma_j \neq \sigma_j}$ where $A_{ij}$ is the adjacency matrix. We also assume the ground truth
	is $\tilde{\sigma} = (0,1,0,1,\dots,)$. Event $A_{12}$ is equivalent to say $\bar{\sigma}=(1,0,0,1,0,1,\dots,)$ is the maximal value of $f(\sigma)$.
	Then $P(A_{12}) = 1 - P(\exists \sigma, s.t. f(\sigma)>f(\bar{\sigma}))$.
	Further:
	\begin{align*}
	P(\exists \sigma, s.t. f(\sigma)>f(\bar{\sigma})) &= P(\exists \sigma, s.t. f(\sigma)>f(\bar{\sigma})| f(\bar{\sigma}) < f(\tilde{\sigma}))P(f(\bar{\sigma}) < f(\tilde{\sigma})) \\
	&+ P(\exists \sigma, s.t. f(\sigma)>f(\bar{\sigma}) | f(\bar{\sigma}) \geq f(\tilde{\sigma}))P(f(\bar{\sigma}) \geq f(\tilde{\sigma}))
	\end{align*}
	We have shown previously $P(f(\bar{\sigma}) \geq f(\tilde{\sigma})) = P(\sum_{i=1}^{n-2} Z_i - X_i \geq 0) = O(n^{-(\sqrt{a} - \sqrt{b})^2})$
	where $Z_i \sim Bern(\frac{b\log n}{n})$ and $X_i \sim Bern(\frac{a \log n}{n})$.
	Therefore,
	$$
	P(\exists \sigma, s.t. f(\sigma)>f(\bar{\sigma})) \leq 1 - O(n^{-(\sqrt{a} - \sqrt{b})^2}) + O(n^{-(\sqrt{a} - \sqrt{b})^2})P(\exists \sigma, s.t. f(\sigma)>f(\bar{\sigma}) | f(\bar{\sigma}) \geq f(\tilde{\sigma}))
	$$
	Using union bound,
	\begin{align*}
	&P(\exists \sigma, s.t. f(\sigma)>f(\bar{\sigma}) | f(\bar{\sigma}) \geq f(\tilde{\sigma}))
	\leq \sum_{\sigma \neq \tilde{\sigma}}P(f(\sigma)>f(\tilde{\sigma})) \\
	&=\sum_{k=1}^{n/4}(\binom{n/2}{k})^2 \exp(-\log n \cdot k(n-2k)(\sqrt{a} - \sqrt{b})^2) = O(n^{2-(\sqrt{a}-\sqrt{b})^2}) = o(1)
	\end{align*}
	Hence, $P(A_{12})\geq O(n^{-(\sqrt{a}-\sqrt{b})^2})$
\end{proof}
\section{Algorithms to solve the data fusion problem}
We estimate the label of each node using only node observation first.
Then for each node, we update its label by a hypothesis testing problem.
\begin{algorithm}
\caption{data fusion algorithm}
\begin{algorithmic}
\REQUIRE graph $G(V,E)$ adjacency matrix $A_{ij}$, node observation $x^{(i)}_{j}$ for $i \in [n], j \in [m]$, mode parameter $p_0, p_1, p, q$
\ENSURE node label $d_i$ for $ i \in [n]$
\FOR{$ i \in [n]$}
\STATE compute the ratio $L_i = \sum_{s=1}^m \log\frac{p_0(x^{(i)}_s)}{p_1(x^{(i)}_s)}$ % initialization of label
\IF{$L_i > 0$}
\STATE $d_i \leftarrow 0$
\ELSE
\STATE $d_i \leftarrow 1$
\ENDIF
\ENDFOR
\WHILE{iteration time does not exceed the maximum}
\FOR{$ i \in [n]$}
\STATE compute the ratio $L_i = \sum_{s=1}^m \log\frac{p_0(x^{(i)}_s)}{p_1(x^{(i)}_s)} + \sum_{s=1, s\neq i}^n
(1-2d_s)[A_{is}\log\frac{p}{q} + (1-A_{is})\log\frac{1-p}{1-q}]$ % initialization of label
\IF{$L_i > 0$}
\STATE $d_i \leftarrow 0$
\ELSE
\STATE $d_i \leftarrow 1$
\ENDIF
\ENDFOR
\ENDWHILE
\end{algorithmic}
\end{algorithm}
For sparse graphs and no node information available, this fusion algorithm is equivalent to
asynchronous label propagation. Since $\log\frac{1-p}{1-q} \approx  0$ 
\begin{thebibliography}{9}
	\bibitem{abbe} Abbe, Emmanuel, Afonso S. Bandeira, and Georgina Hall. "Exact recovery in the stochastic block model." *IEEE Transactions on Information Theory* 62.1 (2015): 471-487.
	\bibitem{yemin} 	Min Ye.
	\newblock Exact recovery and sharp thresholds of stochastic ising block model,
	2020.
\end{thebibliography}
\end{document}