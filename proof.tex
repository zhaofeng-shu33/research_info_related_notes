\documentclass{article}
\usepackage{bm}
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{algorithm,algorithmic}
\usepackage{url}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{hyperref}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}
\DeclareMathOperator{\SDP}{SDP}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{corollary}{Corollary}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\Bern}{Bern}
\newcommand{\A}{\frac{a \log(n)}{n}}
\newcommand{\B}{\frac{b \log(n)}{n}}
\title{Proofs for semi-supervised SBM}
\begin{document}
\maketitle
Notation: $f(n)\dot{=} g(n) \iff \lim_{n\to \infty} \frac{1}{n} \log \frac{f(n)}{g(n)} = 1$.

$SBM(2, p, q)$ with $(p_0, p_1)$, $m=\gamma \log n$. $z_{ij}$ Bernoulli random variable, whether there is an edge between node $i$ and $j$.
$x^{(j)}_{i}$ $j$-th observation at node $i$.

Two cases: $p,q$ are constant and $p=\A, q = \B$.

\section{Constant $p,q$}
\begin{lemma}\label{lem:coupled}
	Suppose $x_i$ is i.i.d. sampled from $p_1$ and $z_i$ is i.i.d. sampled from $Bern(p)$.
	$p > q$
Let $A := \frac{1}{n}\sum_{i=1}^n \log \frac{p_0(x_i)}{p_1(x_i)} \geq \frac{1}{n} \sum_{i=1}(z_i \log\frac{p}{q} +(1-z_i) \frac{\log(1-p)}{\log(1-q)}))$.
Show that $P(A) \dot{=} \exp(-n C)$.
\end{lemma}
\begin{proof}
	To apply Sanov's theorem, we treat $(x_i, z_i)$ jointly sampled from $\mathcal{X} \times \mathcal{Z}$
	and the constraint is 
	\begin{equation}\label{eq:fxz}	
	\frac{1}{n} \sum_{i=1}^n f(x_i, z_i) \geq 0
	\end{equation}	
	where
	$$
	f(x, z) = \log\frac{p_0(x)}{p_1(x)} + z \log\frac{q}{p} + (1-z) \log \frac{1-q}{1-p}
	$$
	Then by Sanov's theorem, we have $C = D(p^*(x,z)|| p_1(x)Bern(p))$
	where $p^*(x,z)$ is the minimizer for $D(p(x,z)|| p_1(x)Bern(p))$ subjected to
	\eqref{eq:fxz}.
	We can rewrite \eqref{eq:fxz} in probability form:
	\begin{equation}\label{eq:probxz}
	\sum_{x\in \mathcal{X}} p(x) \log\frac{p_0(x)}{p_1(x)} + \sum_{z \in \{0,1\}} p(z) \log\frac{p_{Bq}(z)}{p_{Bp}(z)} \geq 0
	\end{equation}
	where $p(x), p(z)$ are the marginal probability mass function.
	Using Lagrange's method, we can get
	\begin{equation}
	p^*(x,z) = C_1 p_0^{\lambda}(x)p_1^{1-\lambda}(x) \cdot C_2 p_{Bq}^{\lambda}(z)p_{Bp}^{1-\lambda}(z)
	\end{equation}
	where $\frac{1}{C_1} = \sum_{x\in\mathcal{X}}p_0^{\lambda}(x)p_1^{1-\lambda}(x) $
	and $\frac{1}{C_2} = \sum_{z\in \{0,1\}} p_{Bq}^{\lambda}(z)p_{Bp}^{1-\lambda}(z)$
	The parameter $\lambda$ is solved by letting the left hand of \eqref{eq:probxz} takes zero.
	That is:
	\begin{equation}\label{eq:constraint}
	C_1 \sum_{x\in \mathcal{X}} p_0^{\lambda}(x)p_1^{1-\lambda}(x)\log\frac{p_0(x)}{p_1(x)} + C_2[p^{1-\lambda}q^{\lambda}\log\frac{q}{p} +
	(1-p)^{1-\lambda}(1-q)^{\lambda}\log\frac{1-q}{1-p} ] = 0
	\end{equation}
	On the other hand,
	\begin{align*}
	C(k) & = D(p(x)|| p_1(x)) + D(p(z) || Bern(p)) \\
	& = C_1\sum_{x\in \mathcal{X}} p_0^{\lambda}(x)p_1^{1-\lambda}(x)\log\frac{C_1  p_0^{\lambda}(x)p_1^{1-\lambda}(x)}{p_1(x)}  \\
	& + C_2[p^{1-\lambda}q^{\lambda}\log\frac{C_2 p^{1-\lambda}q^{\lambda}}{p} +
	(1-p)^{1-\lambda}(1-q)^{\lambda}\log\frac{C_2(1-p)^{1-\lambda}(1-q)^{\lambda}}{(1-p)} ] 
	\end{align*}
	Using \eqref{eq:constraint}, we can simplify $C$ as
	\begin{equation}
	C  =  \log  C_1 + \log C_2
	\end{equation}
	Using $p_0^{\lambda} p_1^{1-\lambda} \leq \lambda p_0 + (1-\lambda) p_1$ we can get $C_1 > 1, C_2 > 1$.

\end{proof}
\begin{corollary}\label{cor:xz}
	Let $p_0 \sim Bern(p), p_1 \sim Bern(q)$ in Lemma \ref{lem:coupled}. Then the region $A$ can be simplified as $\sum_{i=1}^n (x_i - z_i) \geq 0$,
	where $x_i \in Bern(q), z_i \in Bern(p)$ and both are i.i.d. sampled. We can compute $\lambda = \frac{1}{2}$ and the error exponent
	$C = -2\log(\sqrt{pq} + \sqrt{(1-p)(1-q)})$. Below we give another way to prove that $P(A) \dot{=} \exp(-nC)$.
\end{corollary}
\begin{proof}
	We use Chernoff inequality to give a tight upper bound.
	$P(\sum_{i=1}^n (x_i - z_i) \geq 0) \leq \E[\exp(\beta \sum_{i=1}^n (x_i - z_i))]$ where $\beta > 0$.
	$\E[\exp(\beta \sum_{i=1}^n (x_i - z_i))] = (1-q+q e^{\beta})^n(1-p+p e^{-\beta})^n$. Let
	$g(\beta)  = (1-q+q e^{\beta})(1-p+p e^{-\beta})$. $g(0) = 1$ and when $\beta$ is very small,
	$g(\beta) = 1 + (q-p) \beta + o(\beta)$. Therefore, we can choose a proper $\beta$ such that $g(\beta) < 1$.
	Since $\beta$ can take any positive value, now we compute the minimum value of $g(\beta)$ instead.
	the optimal $\beta^*$ satisfies $e^{2\beta} = \frac{(1-q)p}{(1-p)q}$. Now we compute $C' = -\log g(\beta^*) = C$.
	Therefore, $P(A) \leq \exp(-nC)$.
	
	To simplify our discussion, let $X_n = \sum_{i=1}^n X_i, Z_n = \sum_{i=1}^n Z_i$. We have $X_n \sim Binomial(n, q),
	Z_n \sim Binomial(n, p)$ and $X_n, Z_n$ are independent.
	
	On the other hand $P(A) = \sum_{j=1}^n\sum_{k=0}^j P(X_n = j)P(Z_n = k)$.
	We choose a proper $j,k$ such that $P(X_n = j)P(Z_n = k)$ is maximized.
	Since we observe $P(X_n - Z_n = r)$ decreases as $r$ increase ($r\geq 0$).
	The maximum value is taken when $j=k$. On the other hand,
	we should choose:
	$$
	k = n \frac{\sqrt{pq}}{\sqrt{pq} + \sqrt{(1-p)(1-q)}}
	$$
	We have
$$
		P(X_n = k)P(Z_n = k) = \binom{n}{k}^2 p^k (1-p)^{n-k}q^k(1-q)^{n-k}
$$
We will use the inequality $\binom{n}{k} \geq \frac{1}{n+1}\exp(nH(k/n))$.
Then
$$
\log P(X_n = k)P(Z_n = k) \geq -2\log(n+1) + 2n \log(\sqrt{pq} + \sqrt{(1-p)(1-q)})
$$
Therefore, $\log P(A) \geq \log P(X_n = k)P(Z_n = k) \geq -2\log(n+1) + 2n \log(\sqrt{pq} + \sqrt{(1-p)(1-q)})$.
That is $P(A) \geq \frac{1}{(n+1)^2}\exp(-nC)$.
The polynomial term is neglectable compared with the exponential term.
\end{proof}
\begin{remark}
	A third way to prove Corollary \ref{cor:xz} is to use Sanov's theorem directly. Let $Y_i = X_i - Z_i$, which is a random variable
	taking values on $\{-1, 0, 1\}$, its probability vector is given by $Q=[(1-q)p, qp+(1-p)(1-q), q(1-p)]$.
	The event $\sum_{i=1}^n y_i \geq 0 $ can be regarded as $P_{Y^n}(1) - P_{Y^n}(-1) \geq 0$ where the empirical distribution
	of $Y^n$ is summarized from the observed data $\{y_1, \dots, y_n\}$. That is, $P_{Y^n}(r) = \frac{1}{n}\sum_{i=1}^n \bm{1}\{y_i = r\}$.
	Now we can apply Sanov's theorem, the optimal $P_{Y^n}$ to minimize $D_{KL}(P_{Y^n}|| Q)$ is given by
	probability vector $[(1-q)p\exp(-\lambda), qp+(1-p)(1-q), q(1-p)\exp(\lambda)]/C'$ where $C'$ is the normalization constant. The constraint on
	$\lambda$ is that $(1-p)q\exp(\lambda)=(1-q)p\exp(-\lambda)$ (the inequality constraint becomes equality), in which we can solve out
	$P^*_{Z^n} = [\sqrt{pq(1-p)(1-q)}, qp+(1-p)(1-q), \sqrt{pq(1-p)(1-q)}]/C'$ and $C'=(\sqrt{pq}+\sqrt{(1-p)(1-q)})^2$.
	The KL-Divergence of $D_{KL}(P_{Z^n}|| Q)$ can be simplified to $-\log C'$, which is exactly $-2\log (\sqrt{pq}+\sqrt{(1-p)(1-q)})$.
	
	We can also use CramÃ©r's theorem to get the error exponent.
	The moment generating function for $Y_i$ is
	$\varphi(t) = \E[\exp(t Y_i)]
	= qp + (1-p)(1-q) + (1-q)p e^{-t} + q(1-p) e^t$.
	Its conjugate is $\varphi^*(s) = \sup_{t \in \mathbb{R}} (ts - \log \varphi(t))$.
	The exponent is $\varphi^*(0)$ in a sense that $P(\frac{1}{n}\sum_{i=1}^n Y_i\geq 0) \doteq \exp(-n \varphi^*(0))$. $\varphi^*(0)$
	is achieved when $e^t=\frac{(1-q)p}{q(1-p)}$. After simplification,
	we get $\varphi^*(0) = -2\log(\sqrt{pq}+\sqrt{(1-p)(1-q)})$.
\end{remark}
\begin{theorem}\label{thm:2pq}
	For a $\textrm{SBM}(n,2,p, q)$ with $p,q$ as constant, $p > q$. The
	exact recovery error decreases in $\exp(-n (C + o(1)))$
	where $C = -2\log(\sqrt{pq} + \sqrt{(1-p)(1-q)})$.
\end{theorem}
\begin{proof}
	We use maximum likelihood estimation method, let $F$ be the event of the maximum likelihood estimator not coinciding with the ground truth. 
	
	We use $P_n^{(k)}$ to denote the event when there are $k$ pairs wrongly classified. Due to the symmetric property of the problem, $1\leq k \leq \frac{n}{4}$.
	$P_n^{(k)}$ happens when the probability of one $k$ mis-classified configuration has larger probability than that of the ground truth. 
	
	Therefore, we consider one configuration of $k=1$. Let event $A$ be the ground truth and $A_1$ be our chosen event:
	\begin{align*}
	A: & y_1 = 1, y_2 = 0, y_3 = 1, y_4 = 0, \dots, y_{n-1} = 1, y_n = 0 \\
	A_1: & y_1 = 0, y_2 = 1, y_3 = 1, y_4 = 0, \dots, y_{n-1} = 1, y_n = 0 \\	
	\end{align*}
	Then if $P(A) < P(A_1)$, event $F$ will happen. Therefore, we use $P(P(A) < P(A_1))$ as the lower bound of $P(F)$.
	By simplification we get $P(\sum_{i=1}^{n-2} (x_i - z_i) \geq 0) \dot{=} \exp (-nC)$ where $x_i, z_i$ are the same with Corollary \ref{cor:xz}.
	
	For the upper bound, we next show that 
	\begin{equation}
	P(F) \leq \frac{en^2}{4} \exp(-nC).
	\end{equation}
	where $e$ is the natural logarithm number.
	
	When $F$ happens, then $P(A)$ is not the maximum value, there must exists one configuration $P(A') > P(A)$, by the union bound we have:
	\begin{equation}\label{eq:outsidelemmaboundpf5_IT_ub}
	P(F) \leq \sum_{A'} P(P(A') > P(A))
	\end{equation}
	We can classify the right hand by grouping according to the index $k$. That is
	\begin{equation}\label{eq:with14epsilon_14}
	\sum_{A'} P(P(A') > P(A)) = \sum_{k=1}^{n/4} P_{n}^{(k)}
	\end{equation}
	where $P_{n}^{(k)} = \binom{n/2}{k}^2 P(P(A_k) > P(A))$ and $A_k$ is one configuration with $k$ pairs mis-classified.
	Similar to the case $k=1$ we can compute $P(P(A_k) > P(A)) = P(\sum_{i=1}^{k(n-2k)} (x_i - z_i) \geq 0)\dot{=} \exp(-k(n-2k)C)$.
	
	Combining (\ref{eq:outsidelemmaboundpf5_IT_ub}) and (\ref{eq:with14epsilon_14}), we have
	\begin{equation}
	P(F) \leq \sum_{k=1}^{n/4} \binom{n/2}{k}^2 \exp(-k(n-2k)C)
	\end{equation}
	Using the inequality from Lemma \ref{lem:nk}, we have
	\begin{equation}\label{eq:Ff}
	P(F) \leq \sum_{k=1}^{n/4} \exp(-nf(k))
	\end{equation}
	where $f(k)= \frac{2k}{n}\log\frac{2k}{ne} + k(1-\frac{2k}{n})C$.
	By computing $f'(x)= \frac{2}{n} \log \frac{2x}{n} + C - \frac{4Cx}{n}$, $1\leq x \leq \frac{n}{4}$.
	$f'(1) > 0 , f'(\frac{n}{4}) < 0$. Therefore, $f(x)$ increases first and decreases in the interval $[1, \frac{n}{4}]$.
	Comparing $f(1)$ and $f(\frac{n}{4})$ we have $f(1) < f(\frac{n}{4})$. Therefore, $f(k) \geq f(1)$ for $1\leq k \leq \frac{n}{4}$.
	\begin{equation}
	P(F) \leq \frac{n}{4}\exp(-nf(1)) = \exp(-n (C+g(n)))
	\end{equation}
	where $g(n) = -\frac{1}{n}(\log (n/4) + 2 C - 2 \log (2/ne)) = o(1)$
\end{proof}
\begin{theorem}
	In $\textrm{SBM}(n, 2, p, q)$ where $p,q$ are constant, each node has further $n$ observations based on its label.
	That is $ x_{ij} \sim p_{y_i}$ where $y_i = 0 $ or $y_i = 1$. $D(p_0 || p_1) > 0$. Then generally we will have
	the exact recovery error decreases in $\exp(-nC')$. where $C'$ is a constant irreverent with $n$.
\end{theorem}
\begin{proof}
	The proof is similar with that of Theorem \ref{thm:2pq} and we have
	\begin{align}
	C' &= C_1 + C_2  \\
	C_1 &= - 2 \log \sum_{x\in \mathcal{X}} \sqrt{p_0(x)p_1(x)} \\
	C_2 &= -2 \log(\sqrt{pq} + \sqrt{(1-p)(1-q)}) 
	\end{align}
	In short, the error exponent comes from the case when one pair is wrongly classified.
	The event $P(A) < P(A_1)$ can be written as:
	\begin{align*}
	&T_1 = \sum_{x \in \mathcal{X}} p_{X_1}(x) \log \frac{p_0(x)}{p_1(x)}
	+ \sum_{x \in \mathcal{X}} p_{X_2}(x) \log \frac{p_1(x)}{p_0(x)}
	+ \sum_{z \in \{0,1\}} p_{Z_1}(x) \log \frac{p_{B_q}(x)}{p_{B_p}(x)}\\
	&
	+ \sum_{z \in \{0,1\}} p_{Z_2}(x) \log \frac{p_{B_p}(x)}{p_{B_q}(x)} \geq 0
	\end{align*}
	we use Sanov' theorem, and the error exponent is the minimum value of
	$$
	T_2 = D(p_{X_1} || p_1) + D(p_{X_2} || p_0) + D(p_{Z_1} || p_{B_p}) + D(p_{Z_2} || p_{B_q})
	$$
	Using Lagrange's method to minimize $T_2 + \lambda T_1$, we have
	\begin{align*}
	p_{X_1}(x) &= \frac{p_1^{1-\lambda}(x)p_0^{\lambda}(x)}{\sum_{x\in\mathcal{X}}p_1^{1-\lambda}(x)p_0^{\lambda}(x)} \\
	p_{X_2}(x) &= \frac{p_0^{1-\lambda}(x)p_1^{\lambda}(x)}{\sum_{x\in\mathcal{X}}p_0^{1-\lambda}(x)p_1^{\lambda}(x)} \\	
	p_{Z_1}(z) &= \frac{p_{B_p}^{1-\lambda}(x)p_{B_q}^{\lambda}(z)}{\sum_{z\in\{0,1\} }p_{B_p}^{1-\lambda}(z)p_{B_q}^{\lambda}(z)} \\
	p_{Z_2}(z) &= \frac{p_{B_q}^{1-\lambda}(x)p_{B_p}^{\lambda}(z)}{\sum_{z\in\{0,1\} }p_{B_q}^{1-\lambda}(z)p_{B_p}^{\lambda}(z)}	
	\end{align*}
	Taking $\lambda = \frac{1}{2}$ we have $T_1 = 0$ and $T_2 = C'$.
	For $P(P(A_k) > P(A))$ we have it equal to $\exp(-nk C_1 - k(n-2k)C_2)$.
	Similar to the discussion of \eqref{eq:Ff}, we have
	$P(F) \leq \sum_{k=1}^{n/4}P(P(A_1) > P(A)) \dot{=} \exp(-nC_1 - nC_2)$.
\end{proof}
\section{When $p=\A, q=\B$}
\begin{corollary}\label{cor:decrease}
	Now we modify the condition in \ref{lem:coupled} as follows:
	$$
	A := \frac{1}{n}\sum_{i=1}^n \log \frac{p_0(x_i)}{p_1(x_i)} \geq \frac{1}{n} \sum_{i=1}(z_i \log\frac{p}{q} +(1-z_i) \frac{\log(1-p)}{\log(1-q)}) + \epsilon\frac{\log n }{n} [\log \frac{p}{q} - \log \frac{1-p}{1-q}]
	$$
	Take the same assumption in Corollary \ref{cor:xz} that
	$p_0 \sim Bern(p), p_1 \sim Bern(q)$, we now have $\sum_{i=1}^n (x_i - z_i) \geq \epsilon \log n$.
	This problem is interesting when
	$ p = \frac{a \log n }{n} $ and $ q = \frac{b \log n }{n}$. Follow the same procedure as
	in the proof of Lemma \ref{lem:coupled}. The conclusion is that $P(A) \dot{=} n^{-g(a,b,\epsilon)}$.
	where $g(a,b,\epsilon)$ is defined as:
	\begin{equation}\label{eq:gab}
	g(a,b,\epsilon) = a + b - \sqrt{\epsilon^2 + 4ab} + \epsilon \log \frac{\epsilon + \sqrt{\epsilon^2 + 4ab}}{2b}
	\end{equation}
	\begin{remark}
		we have $g'(\epsilon) = \log \frac{\epsilon + \sqrt{\epsilon^2 + 4ab}}{2b}$ and $g''(\epsilon ) = \frac{1}{\sqrt{4ab + \epsilon^2}}$.
		\end{remark}
	\begin{proof}
	Our $\lambda$ is no longer equal to $\frac{1}{2}$ but is the solution to the following equation, modified directly from Equation \eqref{eq:constraint}:
	\begin{equation}\label{eq:Clambda}
	C_1 \sum_{x\in \mathcal{X}} p_0^{\lambda}(x)p_1^{1-\lambda}(x)\log\frac{p_0(x)}{p_1(x)} + C_2 \sum_{z\in \mathcal{X}} p_0^{1-\lambda}(z)p_1^{\lambda}(z)\log\frac{p_1(z)}{p_0(z)}= \epsilon\frac{\log n }{n} [\log \frac{p}{q} - \log \frac{1-p}{1-q}]
	\end{equation}
	And $C= \log C_1 + \log C_2 + \lambda \epsilon\frac{\log n }{n} [\log \frac{p}{q} - \log \frac{1-p}{1-q}]$
	Using the Taylor expansion we have
	$$
	C = (a + b - a^{\lambda}b^{1-\lambda} - a^{1-\lambda}b^{\lambda} + \lambda \epsilon \log\frac{a}{b}) \frac{\log n }{n}
	+ o(\frac{\log n}{n})
	$$
	The $\lambda$ is solved by computing the dominant term on both sides of Equation \eqref{eq:Clambda} and we get
	\begin{equation}
	 a^{\lambda}b^{1-\lambda} - a^{1-\lambda}b^{\lambda} = \epsilon
	\end{equation}
	Solving the above equation we can get the dominant term of $\lambda^*$, as $n\to \infty$, $\lambda_n \to \lambda^*$.
	we have:
	\begin{equation}
	\lambda^*= \frac{
				\log \frac{ 
			\epsilon + \sqrt{\epsilon^2 + 4 ab }	
			} { 2b}
	}
	{\log \frac{a}{b}}
	\end{equation}
	which is larger than $\frac{1}{2}$. we also get the necessary condition by letting $\lambda > 0 \rightarrow \epsilon > b - a$.  Also by letting $\lambda < 1$ we have $\epsilon < a - b$.
	\end{proof}
\end{corollary}
\begin{lemma}\label{lem:nk}
	$\binom{n}{k} < (\frac{ne}{k})^k$
\end{lemma}
\begin{proof}
	$\binom{n}{k} = \frac{n!}{(n-k)!k!} < \frac{n^k}{k!}$.
	Since $e^z = \sum_{k=0}^{\infty} \frac{z^k}{k!} > \frac{z^k}{k!}$,
	let $z=k$ we have $e^k > \frac{k^k}{k!}$.
	Therefore, $\frac{1}{k!} < \frac{e^k}{k^k}$.
	As a result, $\binom{n}{k}  < \frac{n^k}{k!} < \frac{n^k e^k}{k^k}
	= (\frac{ne}{k})^k$
\end{proof}


\begin{lemma}\label{lem:lower_bound}
	Suppose $Z \sim Binomial(n, \frac{b\log n}{n}), X\sim Binomial(n, \frac{a\log n}{n})$.
	For $ \epsilon > b - a$, show that
	\begin{equation}
	P(Z - X \geq \epsilon \log n) \geq \frac{C}{\log^2 n}
	n^{-g(a, b, \epsilon)}
	\end{equation}
	where $g(a,b,\epsilon)$ is given in \eqref{eq:gab},
	and $C$ is a constant.
\end{lemma}
\begin{proof}
	The proof is similar with that of Corollary \ref{cor:xz}.
	Use Chernoff Inequality to prove the upper bound. For the lower bound, we consider
	\begin{align*}
	P(Z - X \geq \epsilon \log n) & \geq P(Z - X = \epsilon \log n) \\
	& = \sum_{\tau \log n = 0}^{n - \epsilon \log n} P(X = \tau \log n) P(Z = (\tau + \epsilon) \log n)
	\end{align*}
	Let
	\begin{align*}
	T(\tau) &= P(X = \tau \log n) P(Z = (\tau + \epsilon) \log n) \\ 
	\log T(\tau) & = \log \binom{n}{\tau \log n} +  \log \binom{n}{(\tau + \epsilon) \log n}
    +  \tau \log n \log p + (\tau + \epsilon) \log n \log q \\
    &+ (n - \tau \log n) \log (1 - p)
    + (n - (\tau + \epsilon) \log n ) \log (1 - q )
	\end{align*}
	We need to choose a $\tau=\tau^*$ such that $T(\tau)$ is maximized. Then
	$ P(Z - X \geq \epsilon \log n) \geq T(\tau^*)$.
	We need a good estimate of $\binom{n}{\tau \log n}$ to write an asymptotic expression of $\log T(\tau)$.
	The estimation formula is given by (for $k\leq \sqrt{N}$)
	$$
	\log \binom{n}{k} \geq k \log N - (k+1) \log (k+1) + k - \log 4
	$$
	Therefore,
	\begin{align*}
	\binom{n}{\tau \log n} &\geq \tau \log^2 n - (\tau \log n + 1)\log(\tau \log n + 1) + \tau \log n  - \log 4 \\
	&\stackrel{(a)}{\geq}  \tau \log^2 n - (\tau \log n + 1)(\log(\tau \log n) + \frac{1}{\tau \log n}) + \tau \log n  - \log 4 \\
	& = \tau \log^2 n - 
	\tau \log n(\log\log n) + \tau (  1 - \log \tau)\log n - \log\log n- (1 + \log 4 + \log \tau) -\frac{1}{\tau \log n}
	\end{align*}
	In (a), we use the inequality:
	$\log(\tau\log n + 1) = \log(\tau\log n) + \log(1+\frac{1}{\tau \log n})
	\leq \log(\tau\log n) + \frac{1}{\tau \log n}$.
	Similarly we can write
	\begin{align*}
	\binom{n}{(\tau + \epsilon) \log n} \geq
	(\tau + \epsilon) \log^2 n&
	-(\tau + \epsilon) \log n(\log\log n) \\
	+ (\tau + \epsilon) (1-\log (\tau + \epsilon) )\log n &- \log\log n 
	- (1 + \log 4 + \log (\tau+\epsilon)) 
	-\frac{1}{(\tau + \epsilon)\log n}
	\end{align*}
	For the other terms, using Taylor expansion we have
	\begin{align*}
		\tau \log n \log p &=
		\tau \log a \log n + \tau \log n (\log\log n)
		- \tau \log^2 n\\
		(n - \tau  \log n ) \log (1 - p )
		&\geq (n - \tau  \log n )(-p - \frac{p^2}{2})
		\geq -a \log n -\frac{a^2 \log^2 n}{2n}
	\end{align*}
	Similar results can be obtained by replacing $\tau$
	with $\tau + \epsilon$. Then
	\begin{align*}
	& \tau \log n \log p + (\tau + \epsilon) \log n \log q 
    + (n - \tau \log n) \log (1 - p)
    + (n - (\tau + \epsilon) \log n ) \log (1 - q ) \\
    =&-(a+b)\log n  + \tau \log n \log\log n - \tau \log^2 n + \tau \log a \log n \\
	&+ (\tau+\epsilon) \log n \log\log n - (\tau+\epsilon) \log^2 n + (\tau+\epsilon) \log b \log n 
	-\frac{(a^2+b^2)\log^2 n}{2n}
    \end{align*}
    To summarize:
    \begin{equation}\label{eq:Tsmall}
    \log T(\tau)  \geq \Big(
    \tau(1-\log \tau) + (\tau + \epsilon)(1-\log(\tau+\epsilon)) + \tau \log a + (\tau+\epsilon) \log b-a-b
    )\Big)\log n -2\log\log n + O(1)
   \end{equation}
   The maximum value on the right-hand side of \eqref{eq:Tsmall} is
   achieved when $\tau = -\frac{\epsilon}{2} + \sqrt{(\frac{\epsilon}{2})^2+ab}$.
   Replacing $\tau$ by its maximum value, we get
   $ \log P(Z-X \geq \epsilon \log n) \geq \log T(\tau^*) = -g(a,b,\epsilon)\log n  + O(\log \log n)$.
   
The above proof is very similar to the proof of Lemma 7 in \cite{abbe}.
\end{proof}
A more careful analysis for the case $\epsilon=0$ gives
more detailed results than Lemma \ref{lem:lower_bound}:
\begin{lemma}
	\begin{equation}\label{eq:detailed_lower_bound}
		P(Z-X\geq 0) \geq (1+o(1))\frac{1}{2\pi \sqrt{ab}\log n}
		n^{-(\sqrt{a}-\sqrt{b})^2}			
	\end{equation}
\end{lemma}
\begin{proof}
	From the proof of Lemma \ref{lem:lower_bound},
	we should choose $\tau=\sqrt{ab} $.
	We first gives the asymptotic expression of
	$P(X = k \log n)$ for general $k$ by Stirling's
	formula, which is
	\begin{equation}\label{eq:xkn}
		P(X=k\log n)
		= (1+o(1))\frac{n^{k\log (a/k) + (k-a)}}{\sqrt{2\pi k \log n}}		
	\end{equation}
	$P(Z-X\geq 0) \geq P(Z = k \log n)
	P(X=k \log n)$ for any $k$.
	Choosing $k=\sqrt{ab}$ and using
	\eqref{eq:xkn} we can get
	\eqref{eq:detailed_lower_bound}.
\end{proof}
\begin{lemma}
	Suppose $m > n, Z \sim
	Binomial(m, \frac{b\log n}{n}), X\sim Binomial(m, \frac{a\log n}{n})$.
	For $ t > \frac{m}{n}(b - a)$, show that
	\begin{equation}
	P(Z - X \geq t \log n) \leq \exp(-\log n \frac{m}{n}\cdot ( g(a, b, \frac{n}{m}t) + O(\frac{\log n}{n})))
	\end{equation}
	where $g(a,b,\epsilon)$ is defined in \eqref{eq:gab}.
\end{lemma}
\begin{proof}
	Similar with the proof of Proposition 5 in \cite{yemin}.
\end{proof}

\begin{lemma}\label{lem:ab2}
	If $\sqrt{a} - \sqrt{b} < \sqrt{2}$, then exact recovery for $SSBM(n, 2, \frac{a \log n}{n}, \frac{b \log n}{n})$
	is impossible.
\end{lemma}
\begin{proof}
	We follow the main idea of Abbe's proof in \cite{abbe} but we give a stronger version.
	We assume $i \in A, j \in B$ where $A$ and $B$ denote two communities.
	We want to show that $P(F) = 1 - o(1)$ by considering $P(F_1)$.
	$F_1: \exists i,j$ such that $E(i, B \backslash \{j\}) + E(j, A \backslash \{i\}) \geq E(i, A \backslash \{i\}) + E(j, B \backslash \{j\})$.
	Then $P(F) \geq P(F_1)$.
	
	Let $F_1(i,j)$ represent the event when $i,j$ are chosen. Then $F_1 = \cup_{i,j}F_1(i,j)$.
	
	Consider $F_1^c = \cap_{i\in A, j\in B} F_1^c(i,j)$. (or $1\leq i \leq n/2, n/2+1 \leq j \leq n$)
	
	Let $H_1\subseteq A, H_2 \subseteq B$ such that $|H_1| = |H_2| = \frac{n}{\log^3 n}$.
	
	To upper bound $P(F_1^c)$ we restrict $i \in H_1, j \in H_2$ and consider
	$G_1(i,j): E(i, B \backslash \{j\}) + E(j, A \backslash \{i\}) \geq E(i, A \backslash H_1) + E(j, B \backslash H_2) + 2\delta(n)$
	where $\delta(n) = \frac{\log n}{\log \log n}$.
	The advantage of considering $G_1(i,j)$ is that $G_1(i,j)$ and $G_1(i', j')$ are independent.
	Let the event $\Delta_1$ represents all nodes in $H_1$ are connected to less than $\delta(n)$ other nodes in $H_1$.
	$\Delta_2$ is defined similarly.
	
	Then we have $F_1^c \Rightarrow \Delta_1^c \cup \Delta_2^c \cup \cap_{i\in A, j\in B}G^c_1(i,j)$
	
	Therefore, we have $P(F_1^c) \leq P(\Delta_1^c) + P(\Delta_2^c) + P(\cap_{i\in A, j\in B}G^c_1(i,j))$.
	We can show that $P(\Delta_1^c) = o(1)$ from Lemma 10 of \cite{abbe}.
	Similar to the analysis in Lemma 4 of \cite{abbe}, we have $P(G_1(i,j)) > n^{-(\sqrt{a} - \sqrt{b})^2}$
	and $P(\cap_{i\in A, j\in B}G^c_1(i,j)) = (1 - P(G_1(i,j)))^{|H_1|^2} \sim \exp(-n^{2 - (\sqrt{a} - \sqrt{b})^2}/\log^6 n) \to 0$.
	We get the conclusion that $P(F_1^c) \to 0$ and $P(F) \to 1$.
	
\end{proof}
\begin{lemma}
	Let $m=\gamma\log n$,
	If $(\sqrt{a} - \sqrt{b})^2 - 2\gamma\log\sum_{x\in\mathcal{X}} \sqrt{p_0(x)p_1(x)} < 2$, then exact recovery for the mixed model with $SSBM(n, m, 2, p_0, p_1, \frac{a \log n}{n}, \frac{b \log n}{n})$
	is impossible.
\end{lemma}
\begin{proof}
	Let $C = - 2\gamma\log\sum_{x\in\mathcal{X}} \sqrt{p_0(x)p_1(x)}$ which is positive.
	The proof is similar with Lemma \ref{lem:ab2} with some modification that we need to add $\sum_{s=1}^m \log \frac{p_0(x^{(i)}_s)}{p_1(x_s^{(i)})}
	+ \sum_{s=1}^m \log \frac{p_1(x^{(i)}_s)}{p_0(x_s^{(i)})}$ to the left hand of the event $F_1(i,j), G_1(i,j)$. ($x_s^{(i)}$ is the sample for the $i$-th node)
	Also we need
	$P(G_1(i,j)) > n^{-(\sqrt{\alpha} - \sqrt{\beta})^2 + C)}$.
\end{proof}
\begin{lemma}\label{lem:UL_bound}
	If $(\sqrt{a}-\sqrt{b})^2-2 > 3a^{1/3}b^{1/3}(a^{1/6}-b^{1/6})^2$ and we consider SSBM $(n, 2, \frac{a\log n}{n}, \frac{b \log n}{n})$, for ML algorithms, the exact recovery error rate $P_e$ satisfies $$
	\lim_{n\to \infty} \frac{\log P_e}{\log n} =2-(\sqrt{a} - \sqrt{b})^2$$.
	More careful analysis gives  $P_e = (\frac{1}{2}+o(1))n^{2-(\sqrt{a} - \sqrt{b})^2}$.
\end{lemma}
\begin{proof}
	In Abbe's original proof (Section 6 of \cite{abbe}), he gave an upper bound of the error rate but that bound is not tight.
	We enhance his results by more careful analysis to show that $P(F) \leq cn^{-2\epsilon}$
	where $\epsilon=2-(\sqrt{a} - \sqrt{b})^2$. The lower bound is established by applying
	Bonferroni inequalities to estimate the probability of the union $n$ events:
	$$
	P(\bigcup_{i=1}^n A_i) \geq \sum_{i=1}^n P(A_i) - \sum_{i<j} P(A_i \cap A_j)
	$$
	We first deal with the lower bound. The summation is $\binom{n}{2}$ pairs
	By Lemma 4 of \cite{abbe}, $P(A_i)$ is lower bounded by $n^{-(\sqrt{a} - \sqrt{b})^2 +o(1)}$.
	Therefore, the first sum is of order $\frac{1}{2}n^{2-(\sqrt{a} - \sqrt{b})^2 +o(1)}$.
	On the other hand, we discuss $P(A_{i,j} \cap A_{r,s})$ according to two cases.
	Firstly is the case when $|\{i,j,r,s\}|=4$. Then $P(A_{i,j} \cap A_{r,s}) \leq
	P(\sum_{i=1}^{2n} X_i - Z_i \geq 0) \leq n^{-2(\sqrt{a} - \sqrt{b})^2} $. There are
	$\binom{n}{4} \leq n^4$ terms. Therefore, the probability sum
	is bounded by $n^{4-2(\sqrt{a} - \sqrt{b})^2}$ which is inferior to $\frac{1}{2}n^{2-(\sqrt{a} - \sqrt{b})^2 +o(1)}$ since $2-(\sqrt{a} - \sqrt{b})^2 < 0$.
	Another case happens when $|\{i,j,r,s\}|=3$. Using Chernoff inequality we have
	$$
	P(\sum_{i=1}^{n} X_i - Z_i + 2\sum_{i=1}^{n/2} (X'_i - Z'_i) \geq 0) \leq \exp(\log n (-\frac{3}{2}(a+b) +be^s+ae^{-s} +\frac{1}{2}(be^{2s}+a^{-2s})))
	$$
	for any $s$.
	We choose $x=e^s$ which minimize $bx+\frac{a}{x}+\frac{1}{2}(bx^2+\frac{a}{x^2})$, which gives $x^3=\frac{a}{b}$.
	Therefore, we obtain
	$$
P(\sum_{i=1}^{n} X_i - Z_i + 2\sum_{i=1}^{n/2} (X'_i - Z'_i) \geq 0) \leq \exp(\frac{3}{2}\log n (-a-b+a^{1/3}b^{2/3}+a^{2/3}b^{1/3}))
$$		
 There are at most $n^3$ such terms. Therefore,
	this sum is upper bounded by  $\exp(\log n (3+\frac{3}{2}(-a-b+a^{1/3}b^{2/3}+a^{2/3}b^{1/3})))$, which is also inferior
	to  $\frac{1}{2}n^{2-(\sqrt{a} - \sqrt{b})^2 +o(1)}$ when the condition
	$(\sqrt{a}-\sqrt{b})^2-2 > 3a^{1/3}b^{1/3}(a^{1/6}-b^{1/6})^2$ holds. We conclude that the lower bound
	is $\frac{1}{2}(1+o(1))n^{2-(\sqrt{a} - \sqrt{b})^2}$
	
	We start with the following inequality, which is Equation 14 in \cite{abbe}.
	\begin{equation}
	P(F) \leq \sum_{k=1}^{n/4} \exp \Big[k\Big( 2\log n - 2 \log 2k + 2 - (\frac{1}{2} - \frac{k}{n})\cdot 4(\frac{a+b}{2} - \sqrt{ab})  \Big)\Big]
	\end{equation}
	Let $\frac{a+b}{2} - \sqrt{ab} = 1 + \epsilon$ where $\epsilon > 0$, $f(k) = -2\log(2k) + 4\frac{k}{n} \log n - (\frac{1}{2} - \frac{k}{n}) 4\epsilon \log n + 2$,
	then we have
	$P(F)\leq \sum_{k=1}^{\sqrt{n}} \exp(kf(k)) + I_2$. where
	\begin{equation}
	I_2 = \sum_{k=\sqrt{n}}^{n/4}\exp(kf(k))
	\end{equation}
	$f'(x) = \frac{-2}{x} + \frac{4(1+\epsilon)\log n}{n}$, for $n$ sufficiently large, we have $f'(1) < 0$, $f'(\sqrt{n})<0$ and $f(x)$ takes maximal value
	at $f(1)$. Therefore, $f(k)\leq f(1)$ for $1\leq k \leq \sqrt{n}$. Then,
	$\sum_{k=1}^{\sqrt{n}} \exp(kf(k)) \leq \frac{\exp(f(1))}{1-\exp(f(1))}$. $\exp(f(1))\sim C n^{-2\epsilon}$ where $C$ is a constant.
	
	Now we want to estimate $I_2$. $f(\frac{n}{4}) > 0$ and $f(x)$ takes minimal value within $[\sqrt{n}, \frac{n}{4}]$.
	$f(\frac{n}{4}) = 2\log 2 + 2 - \log n (1+\epsilon)$, $f(\sqrt{n}) = -2\log 2 - \log n(1+2\epsilon) + \frac{4\epsilon}{\sqrt{n}}\log n + 2 + \frac{4}{n}\log n $.
	Therefore, $f(n/4) > f(\sqrt{n})$. And we have $I_2 \leq \sum_{k=\sqrt{n}}^{n/4} \exp(kf(n/4)) \sim \exp(\sqrt{n}f(n/4))$.
	The dominant term is $n^{-2\epsilon}$, which is exactly $n^{2-(\sqrt{a} - \sqrt{b})^2}$.
	

\end{proof}
\begin{lemma}
	Consider SBM with side information, 
	Let $\gamma = \frac{\log m}{n}$, if $p = a \log n /n$ and $q = b \log n / n$, using maximum likelihood estimator,
	if
	\begin{equation}\label{eq:positive_condition}
	\gamma D_{1/2}(p_0||p_1) + (\sqrt{a} - \sqrt{b})^2-2 > 0
	\end{equation}
	then the error probability
	of exact recovery is bounded by
	\begin{equation}\label{eq:PeMain}
	P_e \leq (1+o(1)) n^{-\left(\gamma D_{1/2}(p_0||p_1) + (\sqrt{a} - \sqrt{b})^2-2\right)}
	\end{equation}
	If the following condition
	\begin{align}
	(\sqrt{a}-\sqrt{b})^2-2 + 2\gamma \min \{D_{\frac{2}{3}}(p_0||p_1), D_{\frac{2}{3}}(p_1||p_0) \} & > 3a^{1/3}b^{1/3}(a^{1/6}-b^{1/6})^2 + 2\gamma D_{1/2}(p_0||p_1)\label{eq:oneC}
	\end{align}
	is satisfied, then we can show that $P_e$ is lower bounded by
	\begin{equation}\label{eq:PeMainL}
	P_e \geq (1+o(1)) n^{-\left(\gamma D_{1/2}(p_0||p_1) + (\sqrt{a} - \sqrt{b})^2-2\right)}
	\end{equation}
	In such case, $P_e=(1+o(1)) n^{-\left(\gamma D_{1/2}(p_0||p_1) + (\sqrt{a} - \sqrt{b})^2-2\right)}$.
\end{lemma}
\begin{proof}
	The proof of the conclusion for the lower bound is similar to that of Lemma \ref{lem:UL_bound}.
	Let $\mu = 2 -\gamma D_{1/2}(p_0||p_1) + (\sqrt{a} - \sqrt{b})^2) $.
	With the exception to discuss the case when $|\{i,j,r,s\}|=3$. In such case, we first suppose the overlap node is from $p_0$, and we need to estimate
	an upper bound for the event
	$$
	A: 2\sum_{i=1}^m  \log \frac{p_1(x_{1i})}{p_0(x_{2i})}
	+ \sum_{i=1}^{2m} \log \frac{p_0(x_{2i})}{p_1(x_{2i})} +\log\frac{a}{b}\left(
	\sum_{i=1}^{n} (X_i - Z_i) + 2\sum_{i=1}^{n/2} (X'_i - Z'_i)\right) \geq 0
	$$
	Using Chernoff inequality, we can write an upper bound of $P(A)$ as
	\begin{align*}
	&P(A) \leq  (\sum_{x\in \mathcal{X}} p_0^{1-2s}p_1^{2s})^m (\sum_{x\in \mathcal{X}} p_1^{1-s}p_0^{s})^{2m} \cdot \\
	&\exp(\log n (-\frac{3}{2}(a+b)+a\exp(-s\log \frac{a}{b})+b\exp(s\log \frac{a}{b}) + \frac{a}{2}\exp(-2s\log \frac{a}{b})+\frac{b}{2}\exp(2s\log \frac{a}{b})))
	\end{align*}
	Let $s=\frac{1}{3}$ we then have
	$$
	P(A)\leq (\sum_{x\in \mathcal{X}} p_0^{1/3}(x)p_1^{2/3}(x))^{3m}\exp(\frac{3}{2}\log n (-a-b+a^{1/3}b^{2/3}+a^{2/3}b^{1/3}))
	$$
	From the condition mentioned in \eqref{eq:oneC},
	we then have
	$$
	P(A) \leq n^{-\mu'-1-(\gamma \cdot D_{1/2}(p_0||p_1) + (\sqrt{a} - \sqrt{b})^2)}
	$$
	where $\mu'>0$ is the difference of two sides in \eqref{eq:oneC}. 
	There are at most $n^3$ such terms, then the summation is bounded by
	$n^{-\mu'-\mu }$,
	which has smaller order than $n^{-\mu}$.
	
	If the overlap node is from $p_1$, similar deduction can be made.
\end{proof}
\begin{theorem}
	Exact recovery for SBM with $p_0, p_1$.
\end{theorem}
\begin{proof}
	For $k=1$, we need to consider the probability of the following event 
	$$
	A: \sum_{i=1}^m \left( \log \frac{p_1(x_{1i})}{p_0(x_{2i})}
	+ \log \frac{p_0(x_{2i})}{p_1(x_{2i})} \right) + \log \frac{a}{b}(\sum_{i=1}^n (z'_i - z_i)) \geq 0
	$$
	where $x_{1i}(x_{2i})$ are sampled from $p_0(p_1)$ and $z'_i(z_i)$ are sampled from $\Bern(\B) (\Bern(\A))$.
	We can use Chernoff inequality to show that $P(A) \leq n^{-\gamma D_{1/2}(p_0||p_1)-(\sqrt{a}-\sqrt{b})^2}$.
	\begin{align*}
	P(A) &\leq \mathbb{E}[\exp( s\sum_{i=1}^m \left( \log \frac{p_1(x_{1i})}{p_0(x_{2i})}
	+ \log \frac{p_0(x_{2i})}{p_1(x_{2i})} \right) + s\log \frac{a}{b}(\sum_{i=1}^n z'_i - z_i) )] \\
	& \stackrel{(a)}{=} (\sum_{x\in \mathcal{X}} p_0^{1-s}p_1^{s})^m (\sum_{x\in \mathcal{X}} p_1^{1-s}p_0^{s})^m
	n^{-a-b+a\exp(-s\log a/b)+b\exp(s\log a/b)}
	\end{align*}
	where $(a)$ follows from independence. Choosing $s=\frac{1}{2}$ we then have 
	$P(A) \leq n^{-\gamma D_{1/2}(p_0||p_1)-(\sqrt{a}-\sqrt{b})^2}$. ($m=\gamma \log n$)
		
	Consideration for general $k$
	
	$$
	\epsilon = -\gamma k\frac{D(X_1^{km} || P_1) - D(X_1^{km} || P_0) + D(X_2^{km} || P_0) - D(X_2^{km} || P_1)}{\log a /b}
	$$
	
	and we minimize
	$$
	k\gamma (D(X_1^{km}||P_1) + D(X_2^{km}||P_0)) + \frac{k(n-2k)}{n} g(\alpha, \beta, \frac{n}{k(n-2k)}\epsilon)
	$$
	Where the function $g(\alpha, \beta, \epsilon)$ is defined as:
	$$
	g(\alpha, \beta, \epsilon) = \begin{cases}
	0 & \epsilon < (b-a) \\
	(\alpha+\beta)-\epsilon \log(\beta) - 2\sqrt{\left( \frac{\epsilon}{2}\right)^2 +\alpha \beta} +\frac{\epsilon}{2} \log \left( \alpha \beta \frac{\sqrt{(\epsilon/2)^2 +\alpha \beta} +\epsilon/2}{\sqrt{(\epsilon/2)^2 +\alpha \beta} -\epsilon/2} \right)& |\epsilon| \leq(a-b) \\
	+\infty & \epsilon > (a-b)
	\end{cases}
	$$
	
	We can use Lagrange multiplier to solve this problem:
	
	We add a term $-\lambda[ \gamma k[D(X_1^{km} || P_1) - D(X_1^{km} || P_0) + D(X_2^{km} || P_0) - D(X_2^{km} || P_1)]+ \log\frac{a}{b}\epsilon]$
	
	Then $g'(\epsilon') = \lambda \log\frac{a}{b}$. where $\epsilon' = \frac{n}{k(n-2k)}\epsilon$
	
	Let $\lambda = \frac{1}{\log(a/b)}\log\frac{\epsilon'+\sqrt{\epsilon'^2+4ab}}{2b}$.
	
	Then $h(\lambda) = -\frac{\log(a/b)\epsilon}{\gamma k}$ from which we can solve $\epsilon(k)=0$.
	
	Then the minimum value equals
	$$
	\Gamma(k) = \frac{k(n-2k)}{n}[a+b-\sqrt{\epsilon'^2 + 4ab}] - k\gamma(\log C + \log C')
	$$
	Let $H(\lambda) = \log C + \log C'$, then $H'(\lambda) = h(\lambda)$
	$$
	h(\lambda)=\frac{1}{C}\sum_{x\in\mathcal{X}} p^{1-\lambda}_1(x)p^{\lambda}_0(x)\log\frac{p_0(x)}{p_1(x)}+\frac{1}{C'}\sum_{x\in\mathcal{X}} p^{1-\lambda}_0(x)p^{\lambda}_1(x)\log\frac{p_1(x)}{p_0(x)}
	$$
	We transform the expression to $\lambda$, we have
	
	\begin{align}
	\epsilon' &= b \exp(\lambda \log(a/b)) - a\exp(-\lambda  \log(a/b)) \\
	\sqrt{\epsilon'^2 + 4ab}  &= b \exp(\lambda  \log(a/b)) + a\exp(-\lambda  \log(a/b)) \\
	\end{align}
	
	Then
	$$
	\Gamma(k) = \frac{k(n-2k)}{n}(a+b) - \Xi[\lambda]
	$$
	where
	$$
	\Xi[\lambda] = \frac{k(n-2k)}{n}(b \exp(\lambda  \log(a/b)) + a\exp(-\lambda \log(a/b))) + \gamma k H(\lambda)
	$$
	$\Xi'[\lambda] = \log(a/b) \epsilon + \gamma k h(\lambda) = 0$.
	
	$\Xi''[\lambda] = \frac{k(n-2k)}{n}\log^2(a/b)(b \exp( \lambda\log(a/b)) + a\exp(-\lambda \log(a/b))) + \gamma k h'(\lambda)>0$
	
	Since
	$$
	h'(\lambda)=\frac{1}{C^2}(C\sum p_1^{1-\lambda}p_0^{\lambda}(\log\frac{p_0}{p_1})^2-(\sum p_1^{1-\lambda}p_0^{\lambda}\log\frac{p_0}{p_1})^2) + \frac{1}{C'^2}(C'\sum p_0^{1-\lambda}p_1^{\lambda}(\log\frac{p_1}{p_0})^2-(\sum p_0^{1-\lambda}p_1^{\lambda}\log\frac{p_1}{p_0})^2)
	$$
	By Cauchy Inequality, $h'(\lambda) > 0$.
	
	Therefore, $\Gamma(k) = \frac{k(n-2k)}{n}(a+b) - \Xi[\lambda^*]$ where $\lambda^*$ is the minimum value of $\Xi$.
	
	We have $\Xi[\lambda^*] \leq \Xi[\frac{1}{2}]$.
	
	Therefore, $\Gamma(k) \geq  \frac{k(n-2k)}{n}(a+b) - \Xi[\frac{1}{2}]$.
	
	We get a lower bound of the minimum value as:
	$$
	\Gamma(k) \geq \frac{k(n-2k)}{n}[a+b-2\sqrt{ab}] - 2\gamma k\log(\sum_{x\in\mathcal{X}}\sqrt{p_0(x)p_1(x)}) =:\Lambda(k)
	$$
	We then analyze the lower bound $\Lambda(k)$.
\end{proof}
\section{SDP relaxation for Mixture Model}
Preliminaries
\begin{equation}\label{eq:g_linear}
g(a,b,\epsilon) \geq  (\sqrt{a} - \sqrt{b})^2 + \frac{\epsilon}{2}\log \frac{a}{b} 
\end{equation}

Consider ML estimation of SBM$(n,2,p,q)$ while each node has further $m$ observations.
Then the likelihood function
$$
L=\prod_{i=1}^n \prod_{j=1}^m p_0^{\sigma_i}(x_i^{(j)})p_1^{1-\sigma_i}(x_i^{(j)})\prod_{(i,j) \in E} p^{\delta(\sigma_i, \sigma_j)}q^{1-\delta(\sigma_i, \sigma_j)}
\prod_{(i,j)\not\in E} (1-p)^{\delta(\sigma_i, \sigma_j)}(1-q)^{1-\delta(\sigma_i, \sigma_j)}
$$
where $\sigma_i \in \{0,1\}$ and $\delta$ is the indicator function.
$\max \log L$ is equivalent to (neglecting some constant term for the given graph $G$)
$$
\max \sum_{i=1}^n \sum_{j=1}^m \sigma_i \log \frac{p_0(x_i^{(j)})}{p_1(x_i^{(j)})}
+\log\frac{p}{q}\sum_{(i,j) \in E} \delta(\sigma_i, \sigma_j)
+\log \frac{1-p}{1-q}\sum_{(i,j)\not\in E} \delta(\sigma_i, \sigma_j)
$$
Let $\kappa = -\log\frac{1-p}{1-q} / \log\frac{p}{q} \sim \frac{a-b}{\log a/b}\frac{\log n}{n}$,
Then we only need to:
$$
\max \frac{1}{\log a/b}\sum_{i=1}^n \sum_{j=1}^m \sigma_i \log \frac{p_0(x_i^{(j)})}{p_1(x_i^{(j)})}
+\sum_{(i,j) \in E} \delta(\sigma_i, \sigma_j)
-\kappa\sum_{(i,j)\not\in E} \delta(\sigma_i, \sigma_j)
$$
Let $\delta(\sigma_i, \sigma_j) = \frac{y_i y_j + 1}{2}, \sigma_i = (y_i+1)/2$ where $y_i,y_j \in \{\pm 1 \}$
We further consider the problem:
$$
\max \frac{1}{\log a/b}\sum_{i=1}^n \sum_{j=1}^m y_i \log \frac{p_0(x_i^{(j)})}{p_1(x_i^{(j)})}
+\sum_{(i,j) \in E} y_i y_j
-\kappa\sum_{(i,j)\not\in E} y_i y_j
$$
Let $h_i = \frac{1}{\log a/b}\sum_{j=1}^m \log \frac{p_0(x_i^{(j)})}{p_1(x_i^{(j)})}$
The data $x_i^{(j)}$ are sampled from $p_0$ if $\sigma_i = 1 (y_i = 1)$. On the contrary,
if $y_i = -1$ the data $x_i^{(j)}$ are sampled from $p_1$.

We multiply the above equation by $2$ and subtract $\frac{1}{2}y^T(J_n-I_n)y$. We then can get
$$
2\sum_{i=1}^n h_iy_i + \sum_{(i,j)\in E} y_i y_j - (2\kappa+1) \sum_{(i,j)\not\in E} y_i y_j
$$

The matrix $B$ is defined as in \cite{abbe} as
\begin{equation}
B_{ij} = \begin{cases}
0 & i=j \\
1 & (i,j) \in E \\
-1 & (i,j) \not\in E
\end{cases}
\end{equation}

Then the problem is $2\max h^T y + \frac{1}{4}y^T B y$ (omit $\kappa=o(1)$).

Following \cite{wang2019tightness}, we use the relaxation
technique to get a SDP problem as follows:
Let $\tilde{B} = \begin{pmatrix} 0 & h^T \\ h & \frac{1}{2}B \end{pmatrix}$
Then we have
\begin{align}
\max\, \tilde{B} \cdot X  &\notag\\
s.t.\, \diag(X) &= 1 \notag\\
 X &\succeq 0 \notag\\
 \sum_{j=1, j\neq i}^{n+1} (X_{ij} + X_{ji})& = 0 \label{eq:sdp}
\end{align}
The number of constraints is $2(n+1)$.

\begin{theorem}\label{thm:sdp}
	Suppose $X^*$ is the optimal solution for \eqref{eq:sdp}.
	Let $\hat{Y}_{\SDP}$ be the estimator of $Y$ where  $(\hat{Y}_{\SDP})_i = X^*_{i+1, 1}$ for $i=1,\dots, n$ .
	%If $\kappa > b\frac{\log n}{n}$
	If $\kappa = 1$
	and
	\begin{align}
	\gamma D_{1/2}(p_0||p_1) + (\sqrt{a} - \sqrt{b})^2 > 2 
	\end{align}
	then $P(\hat{Y}_{\SDP} \neq  Y) = o(1)$.
\end{theorem}

Let $\tilde{g} = (1,g^T)^T$.
We should consider $X^*=\tilde{g}^T \tilde{g}$
as the optimal solution to the above SDP problem, and we can verify $X^*$ satisfies all of the
constraints.

Following Abbe's approach, we consider the dual problem, which has the following form:
\begin{align*}
\min &\sum_{i=1}^{n+1} y_i \\
s.t. &\, \diag\{y_1, \dots, y_n\} + \Xi - \widetilde{B} \succeq 0 
\end{align*}
The symmetric matrix $\Xi$ is defined as 
\begin{equation}
\Xi_{ij} = \begin{cases}
0 & i=j \\
\lambda_i + \lambda_j & i\neq j
\end{cases}
\end{equation}
We choose $y_i, \lambda_i$ for $i=1,\dots, n+1$ which satisfy
the dual constraint and achieve the strong duality.
We define $\mu=\frac{1}{n}\mathbbm{1}_n^T h$ and $\lambda = -u/n$.
Then we choose $\lambda_1=\mu, \lambda_{i+1}=g_i\lambda + \lambda$
and $y_i$ are chosen such that
$(\diag\{y_1, \dots, y_n\} + \Xi - \widetilde{B})\tilde{g}=0$, from which we obtain
\begin{align}
y_1 &= h^T g - n\lambda \\
y_{i+1} & = (h_i +\lambda)g_i + \lambda + \frac{1}{2}\diag\{Bgg^T\}, i = 1, \dots, n
\end{align}
We can also verify that $\sum_{i=1}^{n+1} y_i = 2h^Tg +\frac{1}{2} g^T B g$.
The condition for such solution pair to become optimal is then
\begin{align}
\diag(y) + \Xi - \widetilde{B} & = \begin{pmatrix} h^T (g+\frac{1}{n}\mathbbm{1}_n) & -h^T +(\mu + \lambda)\mathbbm{1}_n^T + \lambda g^T\\
-h+(\mu + \lambda )\mathbbm{1}_n + \lambda g& \Xi_n \end{pmatrix}
\succeq 0 \notag \\
\textrm{ where }\Xi_n & = \diag(hg^T + \frac{1}{2}Bgg^T - \lambda \mathbbm{1}_ng^T)  + 2\lambda J_n -\lambda I_n - \frac{1}{2}B + 2\lambda \Xi'
\end{align}
The matrix $\Xi'$ satisfies $\Xi'_{ij}=g_i + g_j$.

By the construction, $\tilde{g}$ is the eigenvector of $\diag(y) + \Xi - \widetilde{B}$ with eigenvalue $0$.
If all eigenvalues of $\Xi_n$ is larger than zero, then by
Cauchy's Interlace Theorem, all eigenvalues of $\diag(y) + \Xi- \tilde{B}$ is larger than zero.
Let $A$ be the adjacency matrix of $G$
and we define $J_n = \mathbbm{1}_n \mathbbm{1}_n^T $, then $B=2A-J_n+I_n$.
Then we have
\begin{equation}\label{eq:Xi_n}
\Xi_n = \diag(hg^T + Agg^T -\lambda \mathbbm{1}_ng^T)  + (\frac{1}{2} +2\lambda) J_n -\lambda I_n - A + 2\lambda \Xi'
\end{equation}
For any vector $x \in \mathbb{R}^n$ with $\norm{x}=1$, we decompose it as $x=\frac{\beta}{\sqrt{n}} g
+ \sqrt{1-\beta^2} g^{\perp}$ where $g^Tg^{\perp}=0, \beta \in [0,1], \norm{g^{\perp}}=1$, we can expand $x^T \Xi_n x$ as
\begin{align*}
x^T \Xi_n x = \frac{\beta^2}{n} g^T \Xi_n g  &
+		\frac{\beta}{\sqrt{n}}\sqrt{1-\beta^2} g^T \Xi_n g^{\perp}
\\
&+
(1-\beta^2)(g^{\perp})^T \Xi_n g^{\perp} 
\end{align*}
For the first term, using $(\diag(Bgg^T) - B)g=0$ we have
\begin{align*}
g^T \Xi_n g = g^T(h -\lambda I_n) -\lambda  = g^T h-\lambda
\end{align*}
Since $\mathbb{E}[g_ih_i]$ is a positive number of order $O(\log n)$ while $\lambda=O(\frac{\log n}{n})$,
by Sanov's theorem
$P(g^T h < 0)$ decreases exponentially. Next we analyze the second term, Let $\tilde{h}_i
=n\lambda+h_i-\lambda-\lambda g_i$, then
$g^T \Xi_n g^{\perp} = \tilde{h}^T g^{\perp} \geq -\norm{\tilde{h}-\frac{1}{n}(\tilde{h}^Tg)g}$.
The norm can be expanded as:
\begin{align*}
\norm{\tilde{h}-\frac{1}{n}(\tilde{h}^Tg)g}^2
=\norm{\tilde{h}}^2 - \frac{1}{n}(\tilde{h}^Tg)^2
\end{align*}
We define $\hat{g}_1 = \frac{1}{2}(g + \mathbbm{1}_n)$ and $\hat{g}_2 = \frac{1}{2}(-g +\mathbbm{1}_n)$.
Using $\tilde{h}^T\mathbbm{1}_n=\mu$ 
\begin{align*}
\frac{\norm{\tilde{h}}^2}{n} - (\frac{1}{n}\tilde{h}^Tg)^2
&=\frac{\norm{\tilde{h}}^2}{n} - 2\frac{(\tilde{h}^T \hat{g}_1)^2}{n^2} - 2\frac{(\tilde{h}^T \hat{g}_2)^2}{n^2} + \frac{(\tilde{h}^T\mathbbm{1}_n)^2}{n^2} \\
&=\frac{ \sum_{i<j,i,j\in S_1} (\tilde{h}_i - \tilde{h}_j)^2 + \sum_{i<j,i,j\in S_2} (\tilde{h}_i - \tilde{h}_j)^2 }{n^2}+ \frac{\mu^2}{n^2}\\
& = I_1 + I_2 + \frac{\mu^2}{n^2}
\end{align*}
where $I_i=\frac{\sum_{i\in S_i} h_i^2}{n} - 2\frac{(h^T \hat{g}_i)^2}{n^2}$.

If $i\in S_i$, $\mathbb{E}[h_i]=m D_i, \Var[h_i]=m \bar{D}_i, \mathbb{E}[h_i^2]=\Var[h_i]+\mathbb{E}^2[h_i]=m \bar{D}_i+m^2 D^2_i, \Var[h_i^2] \leq m^4 \bar{C}_i$ where
$D_i, C_i,i=1,2$ are constants irrelevant with $n$.
We bound $I_i$ by Chebyshev's inequality
\begin{align*}
P(\Big| \frac{\sum_{i\in S_i} h_i^2}{n} - \frac{1}{2}(m \bar{D}_i + m^2D_i^2) \Big| \geq \log n) & \leq \frac{m^4 \bar{C}_i}{2n\log^2 n} \\
P(\Big| \frac{h^T \hat{g}_i}{n} - \frac{m}{2}D_i\Big| \geq \log^{-1} n) & \leq \frac{m \log^{2} n\bar{D}_i}{2n}
\end{align*}
Therefore, with probability $1-n^{1-o(1)}$, we have
$$
I_i \leq \frac{1}{2}m\bar{D}_i + \frac{1}{2}m^2 D_i^2 + \log n - 2(\frac{1}{2}m D_i - \log^{-1} n)^2 = O(\log n)
$$

Therefore, the second term is lower bounded by
$$
\frac{1}{\sqrt{n}} g^T \Xi_n g^{\perp} \geq -\sqrt{\frac{\norm{\tilde{h}}^2}{n} - (\frac{1}{n}\tilde{h}^Tg)^2} = O(\sqrt{\log n})
$$
For the last term $(g^{\perp})^T \Xi_n g^{\perp} >0$, it is equivalent
to consider the subspace orthogonal to $g$:
$$
\min_{x \perp g, x \in \mathbb{R}^n } x^T \Xi_n x \geq 0
$$
Note that $\mathbb{E}[A] = \frac{p-q}{2}gg^T + \frac{p+q}{2}J_n - pI_n$,
using \eqref{eq:Xi_n}, we can simplify $x^T \Xi_n x$ for $x \perp g$ as
\begin{align*}
x^T \Xi_n x &= x^T \diag( -\lambda \mathbbm{1}_ng^T+hg^T + Agg^T) x  \\
&+ \frac{1}{2}(1-p-q+2\lambda)x^TJ_n x
+ p -\lambda -x^T(A-\mathbb{E}[A])x
\end{align*}
By Theorem 5.2 of \cite{lei2015consistency},
with probability $1-n^{-r}$, $\lambda_{\max}(A-\mathbb{E}[A]) \leq c\sqrt{\log n}$ for some positive constant $r$ and $c$.
\begin{align*}
x^T \Xi_n x \geq \min\{(-\lambda + h_i) g_i + g_i (Ag)_i \} - c \sqrt{\log n}
\end{align*}
The error probability is then bounded by
\begin{align*}
P(\Xi_{ii} \leq c\sqrt{\log n}, \forall 1\leq i \leq n)
\end{align*}

Since $\mathbb{E}[\Xi_{ii}]=O(\log n)$, the order of $P(\Xi_{ii} \leq c\sqrt{\log n})$ is unchanged when we modify $c=0$. Therefore, it is sufficient to consider $\Xi_{ii} \leq 0 $, which is
\begin{align}\label{eq:equiv_condition}
\sum_{j=1}^{n/2} (z_j - z'_j) +g_i( h_i -\lambda )\leq 0
\end{align}
Since $\lambda = O(\frac{\log n}{n})=o(h_i)$, we also can omit it.

Depending on the sign of $g_i$, \eqref{eq:equiv_condition} can be divided into two cases. When $g_i=1$, the samples consisted in $h_i$
follow distribution $p_0$ and the probability is bounded by $n^{-\theta^*_1 + o(1)}$ where
\begin{align}
\theta^*_1 &= \min_{\widetilde{X}_1} \gamma D(p_{\widetilde{X}_1}|| p_0)+ \frac{1}{2} g(a,b, 2\epsilon) \label{eq:theta_star2} \\
\epsilon &= \gamma \frac{D(p_{\widetilde{X}_1} || P_1) - D(p_{\widetilde{X}_1} || P_0) }{\log a /b}
\end{align}
Using linear approximation of $g$ in \eqref{eq:g_linear}, we can show that 
\begin{align*}
\theta^*_1 \geq \frac{1}{2}((\sqrt{a}-\sqrt{b})^2+\gamma D_{1/2}(p_0||p_1)) \\
\geq \frac{1}{2}((\sqrt{a}-\sqrt{b})^2+\gamma D_{1/2}(p_0||p_1))
\end{align*}
Similar results are obtained when $g_i=-1$. Finally,
$P(\Xi_{ii} \leq c\sqrt{\log n}, \exists 1\leq i \leq n)$ is bounded by $n^{1-\theta_1^*+o(1)}$, and Theorem \ref{thm:sdp} follows.
\section{GÃ¤rtner Ellis Theorem}
In this section, we use GÃ¤rtner Ellis Theorem to compute the limit\linebreak
$\lim_{n\to \infty} \frac{1}{\log n} P(\sum_{i=1}^n (X_i - Z_i)  \geq 0)$ for
$X_i \sim \Bern(q), Z_i \sim \Bern(p)$ while $p=\A$ and $q=\B$.

We first give a simplified version of GÃ¤rtner Ellis theorem.
Let $S_n$ be a series of random variables, $\gamma_n >0, \lim_{n\to \infty}\gamma_n \to 0$.
$\Lambda_n(t)=\log \E[\exp(t S_n)]$ represents the log moment generating function of $S_n$ (log-MGF).
We suppose for any $t$, $\Lambda(t) =\lim_{n\to \infty} \frac{1}{\gamma_n}\Lambda_n(\gamma_n t)$
exists. $\Lambda^*$ be the conjugate of $\Lambda$.
Then for any compact set $\Gamma \subset \mathbb{R}$,
we have
\begin{equation}
\lim_{n\to \infty} \frac{1}{\gamma_n}\log P(S_n \in \Gamma) = -\inf_{s \in \Gamma} \Lambda^*(s)
\end{equation}

For a detailed introduction of GÃ¤rtner Ellis theorem, see \cite{div}.

Now we use this theorem to calculate the polynomial error rate.
We choose $\gamma_n= \log n$ and $\gamma_n S_n = \sum_{i=1}^n Y_i$ where $Y_i=X_i - Z_i$.
The distribution of $Y_i$ is given by PMF function $[(1-q)p, qp+(1-p)(1-q), q(1-p)]$ for values $[-1,0,1]$. First we compute
$\Lambda(t) = \lim_{n\to \infty} \frac{n}{\log n} \log \E[\exp(t Y_1)]
=-a-b+be^t  + ae^{-t}$. Then $\Lambda^*(s) = \sup_{t\in\mathbb{R}} st - \Lambda(t)$.
Then
\begin{equation}
\Lambda^*(s) = a+b - \sqrt{s^2 + 4ab} + s\log \frac{s+ \sqrt{s^2 + 4ab}}{2b}
\end{equation}
which is exactly the same as \eqref{eq:gab}.
In this case $\Gamma=[0, +\infty)$. $\Lambda^*(s)$ is an increasing function on $\Gamma$.
$\Lambda^*(0)= (\sqrt{a} - \sqrt{b})^2$.

\section{Necessary condition for exact recovery}
We consider SBM with two communities.
If ML fails with large probability,
all other algorithms will fail too.
This statement is based on Bayesian estimator (the true label
$\sigma$ is a random variable) with the
uniform risk
function: $R(\hat{\sigma}, \sigma) =
\mathbbm{1}_{\norm{\hat{\sigma} - \sigma}\geq \Delta}$. When
$\Delta$ is small, the mean value of this risk function
approximates the probability $P(\hat{\sigma} \neq \sigma)$
while the estimator that minimizes the mean risk
approximates the MAP estimator. Since the prior distribution
of $\sigma$ is uniform, MAP estimator is the same as the ML
estimator.

If we can find
a node pair $i,j$, such that $y_i=1, y_j=-1$ and
exchanging the label assignment of $i,j$ will increase
the maximum likelihood function $p(x|\hat{y})$ ($y$ is the
true labels, $x$ is the edge information),
then ML will fail. This is the probability of error
with one pair (denoted as $\delta_2$).
Actually we can lower bound this error
probability by the square of the error probability
with one node (by Harris inequality).
That is, suppose $\delta_1$ is the probability that
changing one label assignment will increase
$p(x|\hat{y})$.
Then we claim that
$\delta_2 > \delta_1^2$. Then if $\delta_1$ is lower bounded by a positive
constant asymptotically. So is $\delta_2$.

To be more specific, let $S_1,
S_2$ represent the two communities.
$V=S_1\cup S_2$.
Then event $A$ is that
$\exists u \in S_1$ such that $u$ connects
to more edges in $S_2$ than in $S_1$.
And the event $B$ is that
$\exists v \in S_2$
such that $v$ connects
to more edges in $S_1$ than in $S_2$.
Suppose $P(A)>\delta$, by symmetric property
we have $P(B)>\delta$. We will use
Harris inequality \cite{hi} to show $P(A\cap B)
\geq P(A)P(B) > \delta^2$.
We define $\binom{n}{2}$ Bernoulli random
variables $x_{ij}, 1\leq i < j\leq n$. such that
$x_{ij}=\frac{1 - y_iy_j}{2}$ where $y_i \in \{\pm 1\}$
is the $i$-th node label. Then the event
$A$ can be represented as a subset of
$\{(x_{ij})| \exists u, s.t. \sum_{j\neq u} x_{uj} \geq \frac{n}{2}\}$.
(We assume $x_{ij}=x_{ji}$). From this
representation we see clearly that $A$
is an increasing event. The analysis
is the same for $B$, and $B$
is also an increasing event. Then
by Harris inequality, these two
events are positively correlated.
That is $P(A\cap B)
\geq P(A)P(B)$.

In Abbe's proof (Section 5 of \cite{abbe}), he needs to show
$P(A) \geq \frac{2}{3}$ (actually $P(A)\to 1$)
if $\sqrt{a} - \sqrt{b} < \sqrt{2}$).
Then $P(A\cap B) = P(A)
+ P(B) - P(A\cup B) \geq \frac{2}{3}
+ \frac{2}{3} - 1 \geq \frac{1}{3}
$. The lower bound of $P(A\cap B)$ is
also a target in his proof but derived
with different method.
To show $P(A) \to 1$ asymptotically.
We can use the second moment method.
Let $\psi_u$ be the indicator function
that $A_u$ ($u$ connects to more edges
in $S_1$ than in $S_2$) happens, and we consider
$Z=\sum_{u \in S_1} \psi_u$.
Then we claim that there exists $\alpha >
0$ such that
\begin{align}\label{eq:ez}
	\E[Z] &\geq n^{\alpha} \\
	\Var[Z] &\leq C\E^2[Z]
	\label{eq:varz}
\end{align}
and
\eqref{eq:ez} tells us that
$\E[Z]$ is increasing as $n$. Then
by Paley-Zygmund inequality,
$P(Z \geq \theta \E[Z])
\geq \frac{(1-\theta)^2\E^2[Z]}{\Var[Z]
+ \E^2[Z]}$.
Take $\theta = n^{-\alpha}$ we have
$P(Z \geq 1) \geq \frac{1}{C + 1}$.
$Z>1$ is equivalent
to the event $A$.
Notice that by the second moment method,
we did not obtain the strong condition $P(A) \to 1$.

\eqref{eq:ez} holds since $\E[Z]
= n P(A_u)$ while the lower bound
of $P(A_u)$ is obtained
in Lemma \ref{lem:lower_bound} as $\frac{n^{-(\sqrt{a}-\sqrt{b})^2/2}}{\log n}$.
Since $\sqrt{a} - \sqrt{b}<\sqrt{2}$,
we can choose a positive $\alpha>0$ such
that $P(A_u) \geq n^{-1+\alpha}$.

\eqref{eq:varz} is guaranteed when
$\Cov(\xi_u, \xi_v) \leq C P(A_u)^2$
holds for
some constant. Then $\Var[Z]=nP(A_u)+(n^2-n)
\Cov(\xi_u, \xi_v) \leq Cn^2P(A_u)^2=C\E^2[Z]$.

To prove $\Cov(\xi_u, \xi_v) \leq 
C P(A_u)^2$ for $u \in S_1, v\in S_2$,
we construct another
random variable $\xi$, which
indicates whether there is an edge
between $u$ and $v$.
Then $\xi_u$ and $\xi_v$ are conditionally
independent given $\xi$.
By the law of total expectation,
$\Cov(\xi_u, \xi_v)
= \E[\xi_u\xi_v] - \E[\xi_u]
\E[\xi_v]= \E[\E[\xi_u\xi_v|\xi]]
- \E[\E[\xi_u|\xi]]
\E[\E[\xi_v| \xi]]
= \E[\E[\xi_u|\xi]\E[\xi_v|\xi]]
- \E[\E[\xi_u|\xi]]
\E[\E[\xi_v| \xi]]
=\Cov(\E[\xi_u|\xi], \E[\xi_v|\xi])
\leq \sqrt{\Var[\E[\xi_u|\xi]]
\Var[\E[\xi_v|\xi]]}=\Var[\E[\xi_u|\xi]]$.
Notice that $\E[\xi_u|\xi]$ is a random
variable taking two values
$P(A_u | \xi=0)$
and $P(A_u | \xi=1)$.
By law of total expectation we have
\begin{equation}\label{eq:qA_u_mean}
	(1-q) P(A_u | \xi = 0)
	+ q P(A_u | \xi = 1)
	= \E[\E[\xi_u | \xi]]=P(A_u)
\end{equation}
We have the inequality:
$P(A_u | \xi = 0)\leq P(A_u) \leq P(A_u | \xi = 1)$.

If we can show that $P(A_u | \xi = 1) \leq c P(A_u | \xi=0)$
for some constant $c>1$, then from
\eqref{eq:qA_u_mean}, we have
$P(A_u | \xi=0)
\geq \frac{1-cq}{1-q} P(A_u) \geq \frac{1}{2} P(A_u)$.
Therefore, $\Var[\E[\xi_u | \xi]]$ is bounded
by $C P(A_u)^2$.

The inequality $P(A_u | \xi = 1) \leq c P(A_u | \xi=0)$
can be reformulated as 
$\Pr(Y\geq X - 1) \leq c \Pr(Y\geq X)$ while
$X, Y$ are Binomial random variables with parameter
$(n, p), (n, q)$ respectively.

If $\Pr(X=k+1) \leq \beta \Pr(X=k)$, then
\begin{align*}
	\Pr(Y\geq X-1)
	& = \Pr(X=0) + \sum_{k=0}^n \Pr(Y\geq k)\Pr(X=k+1) \\
	& \leq \Pr(Y\geq X)+ \beta\sum_{k=0}^n \Pr(Y\geq k)\Pr(X=k) \\
	& = (1+\beta)\Pr(Y\geq X)
\end{align*}

The inequality $\Pr(X=k+1) \leq \beta \Pr(X=k)$ can be proved directly assuming
$p=\frac{a \log n}{n}$:
\begin{align*}
\frac{\Pr(X=k+1)}{\Pr(X=k)}
= \frac{p}{1-p} \frac{n-k}{k+1} \leq \frac{a}{1-p}\leq 2a
\end{align*}

There is a tiny problem for
$A\cap B \subset F_1$. $F_1$ is the event that
there exists $u \in S_1, v \in S_2$ such that exchanging
the position of $u$ and $v$ can increase
the likelihood function, or equivalently
the number of outer edges (edges between $S_1$ and $S_2$ but not including $e_{uv}$)
is larger than the number of inner edges (within $S_1$ or $S_2$).
When there is no edge between $u$ and $v$, we can
simply add the two conditions together to derive
$F_1$. However, when an edge exists, we can
only get $1 + N(\textrm{outer edges}) \geq N(\textrm{inner edges})$,
which does not imply $-1+N(\textrm{outer edges}) \geq N(\textrm{inner edges})$.
Therefore, we need to show a slightly stronger condition
than $A\cap B$. That is, we must require the event that $u$ and $v$
are non-adjacent occurs simultaneously.

Lemma 7.3 of \cite{mossel} gives techniques to overcome this issue.
In the proof of Lemma 7.3, $S_i$ is divided into three sets,
and the problem is transformed to show that in an Erdos-RÃ©nyi
graph $G(n, \frac{a\log n}{n})$, the probability that it contains $K_{3,3}$
is asymptotically zero. The detail of the first moment method
omitted in that Lemma is given here. Let $X$ be the random
variable that represents the number of $K_{3,3}$ in the
random graph, by Markov's inequality, we have $P(X\neq 0)
=P(X\geq 1)\leq \E[X]$.
$X = \sum X_{ijk,i'j'k'}$ where $X_{ijk,i'j'k'}$ is an
indicator random variable for the existence of $K_{3,3}
$ between $(i,j,k)$ and $(i',j',k')$. Since $K_{3,3}$
has 9 edges,
$\E[X_{ijk,i'j'k'}] = (\frac{a \log n}{n})^9$. The summation
has at most $(\binom{n}{3})^2$ terms, which is of order $O(n^6)$.
Therefore, $\E[X] = O(\frac{\log^9 n}{n^3}) = o(1)$.

We comment that the choice of $K_{3,3}$ is reasonable.
If we choose $K_{2,2}$ then $(\frac{a \log n}{n})^4 (\binom{n}{2})^2$
is not $o(1)$. Therefore, we need $K_{3,3}$ at least.

A final comment is that by the method of Mossel, we
can only show that the error probability of ML algorithm 
has a strictly positive lower bound value. This is weaker
than the conclusion that the error probability tends to 1.

\begin{thebibliography}{9}
	\bibitem{div} KÃ¶nig, Wolfgang. GroÃe Abweichungen: Techniken und Anwendungen. Springer International Publishing, 2020.
	\bibitem{abbe} Abbe, Emmanuel, Afonso S. Bandeira, and Georgina Hall. "Exact recovery in the stochastic block model." *IEEE Transactions on Information Theory* 62.1 (2015): 471-487.
	\bibitem{yemin} 	Min Ye.
	\newblock Exact recovery and sharp thresholds of stochastic ising block model,
	2020.
	\bibitem{wang2019tightness} Wang, Alex L., and Fatma KÄ±lÄ±nÃ§-Karzan. On the tightness of SDP relaxations of QCQPs. Tech. Rep, 2019.
	\bibitem{it} http://matrix.skku.ac.kr/Series-E/Monthly-E.pdf Cauchyâs Interlace Theorem
	\bibitem{lei2015consistency}
	J.~Lei, A.~Rinaldo \emph{et~al.}, ``Consistency of spectral clustering in
	stochastic block models,'' \emph{The Annals of Statistics}, vol.~43, no.~1,
	pp. 215--237, 2015.
	\bibitem{hi} \url{https://ocw.mit.edu/courses/mathematics/18-218-probabilistic-method-in-combinatorics-spring-2019/lecture-notes/MIT18_218S19_ch7.pdf}
	\bibitem{mossel} Mossel, Elchanan, Joe Neeman, and Allan Sly. "Consistency thresholds for the planted bisection model." Proceedings of the forty-seventh annual ACM symposium on Theory of computing. 2015.
\end{thebibliography}
\end{document}